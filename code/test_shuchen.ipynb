{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5847ae3b",
   "metadata": {},
   "source": [
    "# made some changes\n",
    "\n",
    "# change in environment:\n",
    "add statsmodels, seaborn  to pip install \n",
    "\n",
    "\n",
    "1. [x] Introduction about the dynamic foraging task (Jeremiah just has half an hour lecture so maybe we need some explanation or some figures to explain the mechanism) [Shuchen]\n",
    "2. [x] Introduction about generative model, [x] R-W rule [ ] winstayloseshift[Shuchen], and how this code simulates data\n",
    "3. [x] Why and how to perform model simulation [Shuchen]\n",
    "4. [x] How to perform model fitting and parameter recovery [Shuchen]\n",
    "5. [] make things more coherent, check what else would be needed\n",
    "6. [] getting more parameter recovery plots, to make the process more intuitive?\n",
    "7. [] I do not understand how the simple win-stay-lose-shift translate to Han's code\n",
    "8. [] Let's show a parameter recovery plot for RW1972 \n",
    "9. [] And also a model comparison plot between a number of models\n",
    "10/. [] clean up some code in the util function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31858956",
   "metadata": {},
   "source": [
    "<img src=\"./resources/cropped-SummerWorkshop_Header.png\">  \n",
    "\n",
    "<h1 align=\"center\">DAY 4 Workshop SWDB 2025 </h1> \n",
    "<h3 align=\"center\">Friday, August 28th, 2025</h3> \n",
    "<h3 align=\"center\">Fitting Models on Behavioral Data</h3> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5ad2e7",
   "metadata": {},
   "source": [
    "# Model Fitting and Model Comparison \n",
    "\n",
    "## Table of Contents\n",
    "- [The Goal of Model Fitting](#the-goal-of-model-fitting)\n",
    "- [Simulation](#simulation)\n",
    "- [Parameter Estimation](#parameter-estimation)\n",
    "- [Model Comparison](#model-comparison)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc638fcf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99072b08",
   "metadata": {},
   "source": [
    "# Introduction to Computational Model Fitting\n",
    "\n",
    "Fitting computational models to behavioral data allows us to **probe the algorithms underlying behavior**, uncover **neural correlates of latent cognitive variables**, and test mechanistic hypotheses about the mind.\n",
    "\n",
    "The ultimate goal is to use **precise mathematical models** to make sense of rich behavioral data.\n",
    "\n",
    "Behavioral data typically consist of:\n",
    "- **Choices**\n",
    "- **Reaction times**\n",
    "- **Eye movements**\n",
    "- **Neural signals**\n",
    "\n",
    "Computational models describe **how observable variables** (e.g., stimuli, past outcomes, feedback) shape **future behavior**, often through internal latent variables like action values, beliefs, or prediction errors.\n",
    "\n",
    "> *In this way, computational models instantiate algorithmic hypotheses about the processes that generate behavior.*\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Simulation\n",
    "\n",
    "Simulation involves running a model with specified parameters to generate **synthetic (fake) behavioral data**. These surrogate datasets can be analyzed like real data, enabling you to:\n",
    "- Visualize expected behavior under different assumptions\n",
    "- Evaluate the impact of model parameters\n",
    "- Test experimental design sensitivity\n",
    "- Generate falsifiable predictions\n",
    "\n",
    "Simulation sharpens theory by specifying **quantitative predictions** that can be compared with empirical results.\n",
    "\n",
    "> **Examples:** Cohen et al., 1990; Rescorla & Wagner, 1972; Collins & Frank, 2014; Farashahi et al., 2017; Montague et al., 1996; Lee & Webb, 2005.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Parameter Estimation\n",
    "\n",
    "Parameter estimation refers to **inferring model parameters** that best explain observed behavior.\n",
    "\n",
    "This step enables:\n",
    "- **Summarizing individual behavior** with a small set of psychologically meaningful parameters\n",
    "- **Studying individual differences** across participants or groups\n",
    "- **Quantifying the effects** of interventions (e.g., drugs, lesions, experimental conditions)\n",
    "\n",
    "Parameters are typically inferred using **maximum likelihood** or **Bayesian methods**.\n",
    "\n",
    "> **Examples:** Ratcliff, 1978; Wilson et al., 2013; Daw et al., 2011; Frank et al., 2007; Donkin et al., 2016; Gillan et al., 2016.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Model Comparison\n",
    "\n",
    "Once you’ve fit several models to the same dataset, the next step is to determine which model provides the best explanation of the behavior.\n",
    "\n",
    "Model comparison helps us:\n",
    "- Test competing cognitive theories\n",
    "- Reject overfitting models by penalizing complexity\n",
    "- Choose models that **generalize** well beyond the current dataset\n",
    "\n",
    "Common criteria include:\n",
    "- **Akaike Information Criterion (AIC)**\n",
    "- **Bayesian Information Criterion (BIC)**\n",
    "- **Bayes factors**\n",
    "- **Cross-validated log likelihood**\n",
    "\n",
    "This step is especially critical when models make similar qualitative predictions but differ quantitatively in how well they explain the data.\n",
    "\n",
    "> **Examples:** Wilson & Niv, 2011; Daw et al., 2011; Collins & Frank, 2012; Haaf & Rouder, 2017; Donkin et al., 2014.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "Model fitting is a powerful tool for making cognitive theories explicit, testable, and quantitatively precise. It typically involves:\n",
    "\n",
    "1. **Simulation** – to understand what the model predicts.\n",
    "2. **Parameter estimation** – to fit the model to data.\n",
    "3. **Model comparison** – to evaluate competing hypotheses.\n",
    "\n",
    "Together, these steps allow you to go beyond qualitative theorizing and toward a **computationally grounded understanding** of behavior and its neural underpinnings.\n",
    "\n",
    "\n",
    "\n",
    "## Refresh: The Dynamic Foraging Task\n",
    "\n",
    "From flies to primates, all animals must learn about their environment, search for resources, and make adaptive decisions. Foraging—the process of selecting actions to obtain rewards—is a behavior conserved across a wide range of species.\n",
    "\n",
    "The **dynamic foraging task** provides a simple paradigm for studying this behavior. In this task, a head-fixed mouse hears a \"go cue\" signaling the opportunity to perform one of two possible actions: **licking left** or **licking right**.\n",
    "\n",
    "Unbeknownst to the animal, the **probability of receiving a water reward** for licking left or right fluctuates over time. Thus, the animal must learn to infer which action is more rewarding under changing contingencies.\n",
    "\n",
    "---\n",
    "\n",
    "## Q-Learning\n",
    "\n",
    "We quantified learning using **Q-learning**, a reinforcement learning model that estimates the expected value of each action and updates those estimates based on **reward prediction errors** (RPEs) (Bertsekas & Tsitsiklis, 1996; Sutton & Barto, 1998). This model captures how action values are incrementally adjusted over time and allows us to compute trial-by-trial RPEs.\n",
    "\n",
    "---\n",
    "\n",
    "## Model 1: Rescorla-Wagner\n",
    "\n",
    "A classic model of associative learning is the **Rescorla-Wagner rule** (Rescorla & Wagner, 1972). In this model, the expected value of each action is updated based on the discrepancy between received and expected outcomes. Specifically, the value of option $k$ on trial $t$, denoted $Q^k_t$, is updated according to:\n",
    "\n",
    "$$\n",
    "Q^k_{t+1} = Q^k_t + \\alpha (r_t - Q^k_t)\n",
    "$$\n",
    "\n",
    "Here:\n",
    "- $r_t$ is the reward on trial $t$,\n",
    "- $\\alpha \\in [0, 1]$ is the **learning rate**, controlling how strongly the prediction error $(r_t - Q^k_t)$ updates the value,\n",
    "- $Q^k_0$ is typically initialized to zero or treated as a free parameter.\n",
    "\n",
    "To translate learned values into choices, we use the **softmax decision rule**, which models the stochastic selection of actions based on their values. The probability of choosing action $k$ is given by:\n",
    "\n",
    "$$\n",
    "p^k_t = \\frac{\\exp(\\beta Q^k_t)}{\\sum_{i=1}^K \\exp(\\beta Q^i_t)}\n",
    "$$\n",
    "\n",
    "Here:\n",
    "- $\\beta$ is the **inverse temperature** parameter. Higher $\\beta$ leads to more deterministic (greedy) choices, while lower $\\beta$ encourages more exploratory behavior.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "\n",
    "By combining the learning rule and decision rule, we obtain a simple yet powerful model of behavior with **two free parameters**: the learning rate $\\alpha$ and the inverse temperature $\\beta$. In our general notation, the model is parameterized by:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\theta}_3 = (\\alpha, \\beta)\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## Model 2: Win-Stay Lose-Shift\n",
    "\n",
    "The **Win-Stay Lose-Shift (WSLS)** strategy is a simple, psychologically plausible decision-making rule that does not rely on estimating action values. Instead, it operates directly on the outcome of the previous trial.\n",
    "\n",
    "The idea is straightforward:\n",
    "- If the agent **received a reward** for choosing an action on the previous trial (a **win**), it **repeats** that action.\n",
    "- If the agent **did not receive a reward** (a **loss**), it **switches** to the alternative action on the next trial.\n",
    "\n",
    "We can express this as a probabilistic rule using two parameters:\n",
    "- $p_{\\text{stay}|\\text{win}}$: the probability of staying with the same choice after a win,\n",
    "- $p_{\\text{shift}|\\text{loss}}$: the probability of switching after a loss.\n",
    "\n",
    "Let $c_{t-1}$ denote the action chosen on the previous trial, and $r_{t-1} \\in \\{0, 1\\}$ be the reward received. Then, the probability of choosing action $k$ on trial $t$ is:\n",
    "\n",
    "$$\n",
    "p^k_t = \n",
    "\\begin{cases}\n",
    "p_{\\text{stay}|\\text{win}}, & \\text{if } c_{t-1} = k \\text{ and } r_{t-1} = 1 \\\\\n",
    "1 - p_{\\text{shift}|\\text{loss}}, & \\text{if } c_{t-1} = k \\text{ and } r_{t-1} = 0 \\\\\n",
    "1 - p_{\\text{stay}|\\text{win}}, & \\text{if } c_{t-1} \\neq k \\text{ and } r_{t-1} = 1 \\\\\n",
    "p_{\\text{shift}|\\text{loss}}, & \\text{if } c_{t-1} \\neq k \\text{ and } r_{t-1} = 0 \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "This rule defines a **reactive strategy** that bases choices solely on the most recent outcome. It is memoryless and value-free, making it distinct from reinforcement learning models like Rescorla-Wagner or Q-learning.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "\n",
    "The WSLS model has two free parameters: the probability of repeating after a win ($p_{\\text{stay}|\\text{win}}$) and the probability of switching after a loss ($p_{\\text{shift}|\\text{loss}}$). In our general notation, the model is parameterized by:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\theta}_4 = \\left(p_{\\text{stay}|\\text{win}},\\; p_{\\text{shift}|\\text{loss}} \\right)\n",
    "$$\n",
    "\n",
    "## Model Simulation \n",
    "\n",
    "\n",
    "Once you have an experimental design and a set of computational models, a really important step is to create fake, or surrogate data (Palminteri et al., 2017). That is, you should use the models to simulate the behavior of participants in the experiment, and to observe how behavior changes with different models, different model parameters, and different variants of the experiment. This step will allow you to refine the first two steps: confirming that the experimental design elicits the behaviors assumed to be captured by the computational model. To do this, here are some important steps.\n",
    "\n",
    "## Generating Surrogate Data for Model Validation\n",
    "\n",
    "1. **Specify the Generative Model**  \n",
    "   - Choose the model(s) $M_1, M_2, \\dots$ you want to test (e.g., Q-learning, Rescorla–Wagner, WSLS).  \n",
    "   - Decide which latent variables drive behavior (e.g., action values $Q_t$, prediction errors).\n",
    "\n",
    "2. **Define Parameter Priors / Grids**  \n",
    "   - For each model, pick realistic parameter ranges or priors:  \n",
    "     - Learning rate $\\alpha \\in [0.05, 0.9]$  \n",
    "     - Inverse temperature $\\beta \\in [0.5, 10]$  \n",
    "     - WSLS stay/shift probabilities $p_{\\text{stay}}, p_{\\text{shift}} \\in [0, 1]$  \n",
    "   - Use either a **grid**, **random uniform**, or **hierarchical prior** sampling scheme.\n",
    "\n",
    "3. **Simulate Behavioral Data**  \n",
    "   - For every parameter draw $\\boldsymbol{\\theta}^{(i)}$:  \n",
    "     1. Initialize model state (e.g., $Q^k_0 = 0$).  \n",
    "     2. Loop over trials $t = 1 \\dots T$:  \n",
    "        - Sample choice $c_t \\sim p(c_t \\mid d_{1:t-1}, \\boldsymbol{\\theta}^{(i)}, M)$  \n",
    "        - Sample outcome $r_t$ from the task’s reward schedule  \n",
    "        - Update model states.  \n",
    "   - Store trial-wise choices, outcomes, and latent variables.\n",
    "\n",
    "4. **Compute Diagnostic Summaries**  \n",
    "   - Reaction-time or choice-probability curves  \n",
    "   - Learning curves (e.g., moving-window choice of optimal action)  \n",
    "   - Distribution of prediction errors, switch rates, etc.  \n",
    "   Use these summaries to confirm the simulated behavior matches the qualitative patterns you expect.\n",
    "\n",
    "5. **Parameter-Recovery Check**  \n",
    "   - Fit the same model back to the simulated data.  \n",
    "   - Correlate recovered parameters $\\hat{\\boldsymbol{\\theta}}$ with the ground-truth $\\boldsymbol{\\theta}^{(i)}$.  \n",
    "   - Good recovery (slope ≈ 1, $R^2$ high) indicates your model + fitting procedure can, in principle, retrieve true parameters.\n",
    "\n",
    "6. **Model-Recovery / Confusability Test**  \n",
    "   - Fit *all* candidate models to each surrogate dataset.  \n",
    "   - Evaluate with AIC/BIC or Bayes factors:  \n",
    "     $$ \\text{AIC} = -2\\log\\hat{L} + 2k,\\quad \\text{BIC} = -2\\log\\hat{L} + k\\log T. $$  \n",
    "   - Ideally, the data generated by model $M_i$ is most strongly favored when you fit $M_i$ (diagonal dominance in a confusion matrix).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Basic Idea of Model Fitting\n",
    "\n",
    "### ▪️ Maximum Likelihood Estimate\n",
    "\n",
    "To fit a model to behavioral data, we estimate the parameters $\\boldsymbol{\\theta}_M$ that maximize the likelihood of the observed data under model $M$. This is typically done using **maximum likelihood estimation (MLE)**:\n",
    "\n",
    "$$\n",
    "\\hat{\\boldsymbol{\\theta}}^{\\text{MLE}}_M = \\arg\\max_{\\boldsymbol{\\theta}_M} \\log p(d_{1:T} \\mid \\boldsymbol{\\theta}_M, M) = \\arg\\max_{\\boldsymbol{\\theta}_M} \\sum_{t=1}^{T} \\log p(c_t \\mid d_{1:t-1}, \\boldsymbol{\\theta}_M, M)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### ▪️ Data Input: $d_{1:T}$\n",
    "\n",
    "The observed data includes:\n",
    "- **Choice history**: $c_t \\in \\{\\text{L}, \\text{R}\\}$\n",
    "- **Reward history**: rewards associated with left and right actions:\n",
    "  - $r_{R,t}$: reward received when choosing right at time $t$\n",
    "  - $r_{L,t}$: reward received when choosing left at time $t$\n",
    "\n",
    "---\n",
    "\n",
    "### ▪️ Predictive Likelihood\n",
    "\n",
    "At each time step $t$, the model $M$ with parameters $\\boldsymbol{\\theta}_M$ uses the full **history up to $t - 1$**, i.e., $d_{1:t-1}$, to compute the likelihood of the next choice $c_t$:\n",
    "\n",
    "$$\n",
    "L_t = p(c_t \\mid d_{1:t-1}, \\boldsymbol{\\theta}_M, M)\n",
    "$$\n",
    "\n",
    "This likelihood score (e.g., 0.63) reflects how well the model predicts the observed choice on trial $t$.\n",
    "\n",
    "🟥 **Important**: This is a **predictive** model fit — it evaluates how well the model predicts observed behavior. It is **not generative**, i.e., we do not simulate new behavior forward in time.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Model Comparison\n",
    "\n",
    "When evaluating which model best explains behavioral data, we compare models not only by how well they fit the data (i.e., likelihood), but also by penalizing model complexity. Two widely used criteria are:\n",
    "\n",
    "### ▪️ Information Criteria\n",
    "\n",
    "- **Akaike Information Criterion (AIC)**:\n",
    "  \n",
    "  $$\n",
    "  \\text{AIC} = -2 \\log \\hat{L} + 2k\n",
    "  $$\n",
    "\n",
    "- **Bayesian Information Criterion (BIC)**:\n",
    "\n",
    "  $$\n",
    "  \\text{BIC} = -2 \\log \\hat{L} + k \\log T\n",
    "  $$\n",
    "\n",
    "  Where:\n",
    "  - $\\hat{L}$ is the maximum likelihood\n",
    "  - $k$ is the number of free parameters in the model\n",
    "  - $T$ is the number of data points (e.g., trials)\n",
    "\n",
    "- These criteria balance **model fit** with **model complexity**. Lower AIC/BIC values indicate a better model.\n",
    "\n",
    "### ▪️ Approximate Posterior over Models\n",
    "\n",
    "The relative probability of a model given the data can be approximated as:\n",
    "\n",
    "$$\n",
    "p(M \\mid \\text{data}) \\propto \\exp\\left(-\\frac{\\text{IC}}{2}\\right)\n",
    "$$\n",
    "\n",
    "This allows us to derive:\n",
    "- **Relative log-likelihood**: $\\log_{10} \\frac{p(\\text{model})}{p(\\text{best model})}$\n",
    "- **Model weights**: normalized probabilities over models\n",
    "\n",
    "---\n",
    "\n",
    "### ▪️ Interpretation of the Plot\n",
    "\n",
    "- Each **row** corresponds to a candidate model, defined by its parameter set.\n",
    "- The **ground truth** model in simulation is highlighted (e.g., RW1972).\n",
    "- **Bars from left to right** show:\n",
    "  - **Likelihood per trial** (higher = better)\n",
    "  - **AIC and BIC scores** (lower = better)\n",
    "  - **Relative log-likelihood**\n",
    "  - **Model weight** (posterior probability of the model)\n",
    "\n",
    "✅ Better models are found toward the **right side** of the likelihood plot and the **left side** of the AIC/BIC and log-likelihood plots.\n",
    "\n",
    "🟦 AIC (blue) and 🟧 BIC (orange) sometimes rank models differently depending on penalty strength.\n",
    "\n",
    "---\n",
    "\n",
    "### ▪️ Summary\n",
    "\n",
    "By comparing both likelihood and complexity-penalized scores, we can determine:\n",
    "- Which model best accounts for the data,\n",
    "- Whether model differences are **decisive** (e.g., by a threshold on log-likelihood or posterior weight),\n",
    "- And whether more complex models actually yield better predictive power.\n",
    "\n",
    "📌 **Ground truth** recovery (e.g., identifying RW1972) helps validate the fitting pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f636d05f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9063d4b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "216225ef",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2162e7e",
   "metadata": {},
   "source": [
    "# Simulation and parameter recovery of dynamic foraging task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "175b622e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the task to uncoupled \n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "from aind_behavior_gym.dynamic_foraging.task import UncoupledBlockTask\n",
    "from utils_model_recovery import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d016ab",
   "metadata": {},
   "source": [
    "## Task simulation\n",
    "1. Task environment initialization\n",
    "2. agent initialization\n",
    "3. simulate task interaction with the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e041319d",
   "metadata": {},
   "source": [
    "### Environment initialization\n",
    "First we want to initialize a world environment which will interact with the agent to give feedback, including reward/force switch etc. This is like the rules for the task. \n",
    "\n",
    "The `UncoupledBlockTask` is a wrapped environment where reward probabilities for each action (e.g., left or right) change **independently** across blocks of trials.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07c89f7",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b> Class UncoupledBlockTask explained </b></summary>\n",
    "\n",
    "**Overview**\n",
    "\n",
    "- The `UncoupledBlockTask` class implements a dynamic foraging environment where reward probabilities for left and right choices change **independently** across blocks of trials.\n",
    "\n",
    "**Initialization Arguments**\n",
    "- `reward_baiting`: `bool = False`\n",
    "  - Whether rewards persist until collected\n",
    "  - `True`: Uncollected rewards remain available\n",
    "  - `False`: Rewards are generated fresh each trial\n",
    "\n",
    "- `allow_ignore`: `bool = False`\n",
    "  - Whether agent can skip trials\n",
    "  - `True`: Adds \"ignore\" as third action option\n",
    "  - `False`: Agent must choose left or right\n",
    "\n",
    "- `num_arms`: `int = 2`\n",
    "  - Number of choice options (typically 2 for left/right)\n",
    "\n",
    "- `num_trials`: `int = 1000`\n",
    "  - Total number of trials in the session\n",
    "\n",
    "- `seed`: `int = None`\n",
    "  - Random seed for reproducibility\n",
    "\n",
    "- `rwd_prob_array`: `List[float] = [0.1, 0.5, 0.9]`\n",
    "  - Available reward probabilities for block assignment\n",
    "  - Each new block randomly selects from this array\n",
    "\n",
    "- `block_min`: `int = 20`\n",
    "  - Minimum block length in trials\n",
    "\n",
    "- `block_max`: `int = 35`\n",
    "  - Maximum block length in trials\n",
    "\n",
    "- `persev_add`: `bool = True`\n",
    "  - Enable anti-perseveration mechanism\n",
    "\n",
    "- `perseverative_limit`: `int = 4`\n",
    "  - Number of consecutive choices on min-prob side to trigger anti-persev\n",
    "\n",
    "- `max_block_tally`: `int = 4`\n",
    "  - Maximum consecutive blocks one side can be better before forced balancing\n",
    "\n",
    "**Key Methods**\n",
    "\n",
    "- `reset()`: Initialize new session with fresh block schedule\n",
    "- `step(action)`: Execute one trial and return (observation, reward, done, info)\n",
    "- `generate_new_trial()`: Create reward probabilities for next trial\n",
    "- `get_choice_history()`, `get_reward_history()`: Extract behavioral data\n",
    "- `get_p_reward()`: Get reward probability matrix for analysis\n",
    "- `plot_reward_schedule()`: Visualize block structure and choice patterns\n",
    "\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2944ae",
   "metadata": {},
   "source": [
    "#### Create example environment / task rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d338755",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = UncoupledBlockTask(reward_baiting=True, num_trials=1000, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a5ffec",
   "metadata": {},
   "source": [
    "- `rwd_prob_array=[0.1, 0.5, 0.9]`: Each option's reward probability is independently selected from the set [0.1, 0.5, 0.9] at the start of each block  \n",
    "- `reward_baiting=True`: If a reward is available but not collected, it remains \"baited\" until chosen  \n",
    "- `allow_ignore=False, persev_add=True, perseverative_limit=4`: The agent must make a choice on every trial (ignoring is not allowed), and an anti-perseveration mechanism ensures that if the agent repeats the same choice for four or more consecutive blocks, the task will force a switch to prevent excessive bias  \n",
    "- `max_block_tally=4`: No more than four consecutive blocks can favor the same side, maintaining a balanced and challenging environment for adaptive decision-making  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e632b7",
   "metadata": {},
   "source": [
    "### Agent initialization\n",
    "Next we want to initialize an agent which will interact with the task environment following its internal belief\n",
    "\n",
    "`ForagerQLearning` is a sophisticated reinforcement learning agent that implements Q-learning for dynamic foraging tasks. It's designed for fitting to behavioral data and includes many advanced features for model comparison and analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe7b457",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b> class ForagerQLearning explained </b></summary>\n",
    "\n",
    "**Core Functionality:**\n",
    "\n",
    "- **Q-Value Learning**: Incrementally updates action-value estimates based on reward history\n",
    "- **Flexible Learning Rates**: Supports different learning rates for rewarded vs unrewarded outcomes\n",
    "- **Choice Kernels**: Models choice history effects on decision-making\n",
    "- **Action Selection**: Multiple strategies including softmax and epsilon-greedy\n",
    "- **Forgetting Mechanisms**: Optional decay of unchosen option values\n",
    "- **Parameter Fitting**: Maximum likelihood estimation with cross-validation support\n",
    "\n",
    "\n",
    "**Core Configuration Parameters:**\n",
    "\n",
    "- `number_of_learning_rate`: `Literal[1, 2] = 2`\n",
    "  - Controls learning rate structure\n",
    "  - `1`: Single learning rate for all outcomes\n",
    "  - `2`: Separate rates for rewarded (`learn_rate_rew`) and unrewarded (`learn_rate_unrew`) outcomes\n",
    "\n",
    "- `number_of_forget_rate`: `Literal[0, 1] = 1`\n",
    "  - Controls forgetting mechanism\n",
    "  - `0`: No forgetting of unchosen options\n",
    "  - `1`: Includes `forget_rate_unchosen` parameter\n",
    "\n",
    "- `choice_kernel`: `Literal[\"none\", \"one_step\", \"full\"] = \"none\"`\n",
    "  - Choice history influence on decisions\n",
    "  - `\"none\"`: No choice history effects\n",
    "  - `\"one_step\"`: Only immediate previous choice (Bari2019 style)\n",
    "  - `\"full\"`: Exponentially weighted choice history with learnable parameters\n",
    "\n",
    "- `action_selection`: `Literal[\"softmax\", \"epsilon-greedy\"] = \"softmax\"`\n",
    "  - Decision strategy\n",
    "  - `\"softmax\"`: Probabilistic selection with inverse temperature parameter\n",
    "  - `\"epsilon-greedy\"`: Exploration with fixed epsilon probability\n",
    "\n",
    "**Parameter Specifications:**\n",
    "\n",
    "- `params`: `dict = {}`\n",
    "  - Initial model parameters (auto-generated based on configuration)\n",
    "  - Learning rates: `learn_rate`, `learn_rate_rew`, `learn_rate_unrew` (0.0-1.0)\n",
    "  - Forget rates: `forget_rate_unchosen` (0.0-1.0)\n",
    "  - Bias: `biasL` (left side bias, -5.0 to 5.0)\n",
    "  - Action selection: `softmax_inverse_temperature` (0.0-100.0) or `epsilon` (0.0-1.0)\n",
    "  - Choice kernel: `choice_kernel_relative_weight`, `choice_kernel_step_size` (0.0-1.0)\n",
    "\n",
    "- `**kwargs`: Additional arguments passed to base class (e.g., `seed` for reproducibility)\n",
    "\n",
    "**Key Methods:**\n",
    "\n",
    "- `perform(task)`: Simulate agent behavior on a foraging task\n",
    "- `fit(choice_history, reward_history)`: Fit model parameters to behavioral data\n",
    "- `act(observation)`: Select action based on current Q-values\n",
    "- `learn(observation, action, reward, next_observation, done)`: Update Q-values after action\n",
    "- `plot_session()`: Visualize behavioral session and internal states\n",
    "- `get_choice_history()`, `get_reward_history()`: Extract behavioral data\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4c39d2",
   "metadata": {},
   "source": [
    "#### Create example agent\n",
    "This agent implements the Q-learning algorithm for dynamic foraging tasks. Q-learning is a model-free reinforcement learning method where the agent maintains an estimate of the expected value (Q-value) for each action. The agent selects actions using an \"epsilon-greedy\" policy: with probability $1-\\epsilon$, it chooses the action with the highest Q-value (exploitation), and with probability $\\epsilon$, it selects an action at random (exploration). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b8981f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "forager = ForagerQLearning(number_of_learning_rate=1,number_of_forget_rate=0,choice_kernel=\"none\",action_selection=\"epsilon-greedy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1806f7b0",
   "metadata": {},
   "source": [
    "### Simulate agent interacting with the task environment \n",
    "i.e. an artificial agent performing the dynamic foraging task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aad3b28",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "choose_ps() got an unexpected keyword argument 'rng'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Run the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mforager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Capture the results\u001b[39;00m\n\u001b[1;32m      5\u001b[0m ground_truth_params \u001b[38;5;241m=\u001b[39m forager\u001b[38;5;241m.\u001b[39mparams\u001b[38;5;241m.\u001b[39mmodel_dump()\n",
      "File \u001b[0;32m~/GitHub/SWDB_2025_Dynamical_Systems/code/utils_model_recovery.py:791\u001b[0m, in \u001b[0;36mDynamicForagingAgentMLEBase.perform\u001b[0;34m(self, task)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrial \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask\u001b[38;5;241m.\u001b[39mtrial  \u001b[38;5;66;03m# Ensure the two timers are in sync\u001b[39;00m\n\u001b[1;32m    790\u001b[0m \u001b[38;5;66;03m# -- Agent performs an action\u001b[39;00m\n\u001b[0;32m--> 791\u001b[0m choice, choice_prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[38;5;66;03m# -- Environment steps (enviromnet's timer ticks here!!!)\u001b[39;00m\n\u001b[1;32m    794\u001b[0m _, reward, task_done, _, _ \u001b[38;5;241m=\u001b[39m task\u001b[38;5;241m.\u001b[39mstep(choice)\n",
      "File \u001b[0;32m~/GitHub/SWDB_2025_Dynamical_Systems/code/utils_model_recovery.py:1563\u001b[0m, in \u001b[0;36mForagerQLearning.act\u001b[0;34m(self, _)\u001b[0m\n\u001b[1;32m   1553\u001b[0m     choice, choice_prob \u001b[38;5;241m=\u001b[39m act_softmax(\n\u001b[1;32m   1554\u001b[0m         q_value_t\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_value[:, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrial],\n\u001b[1;32m   1555\u001b[0m         softmax_inverse_temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams\u001b[38;5;241m.\u001b[39msoftmax_inverse_temperature,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1560\u001b[0m         rng\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrng,\n\u001b[1;32m   1561\u001b[0m     )\n\u001b[1;32m   1562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction_selection\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepsilon-greedy\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1563\u001b[0m     choice, choice_prob \u001b[38;5;241m=\u001b[39m \u001b[43mact_epsilon_greedy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mq_value_t\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_value\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1566\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbias_terms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbiasL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1567\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# -- Choice kernel --\u001b[39;49;00m\n\u001b[1;32m   1568\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchoice_kernel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchoice_kernel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1569\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchoice_kernel_relative_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchoice_kernel_relative_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1570\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1571\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1573\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m choice, choice_prob\n",
      "File \u001b[0;32m~/GitHub/SWDB_2025_Dynamical_Systems/code/utils_model_recovery.py:493\u001b[0m, in \u001b[0;36mact_epsilon_greedy\u001b[0;34m(q_value_t, epsilon, bias_terms, choice_kernel, choice_kernel_relative_weight, rng)\u001b[0m\n\u001b[1;32m    490\u001b[0m     choice_prob[argmax_Q] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m epsilon \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;66;03m# -- Choose action --\u001b[39;00m\n\u001b[0;32m--> 493\u001b[0m choice \u001b[38;5;241m=\u001b[39m \u001b[43mchoose_ps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchoice_prob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrng\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m choice, choice_prob\n",
      "\u001b[0;31mTypeError\u001b[0m: choose_ps() got an unexpected keyword argument 'rng'"
     ]
    }
   ],
   "source": [
    "# Run the model\n",
    "forager.perform(task)\n",
    "\n",
    "# Capture the results\n",
    "ground_truth_params = forager.params.model_dump()\n",
    "ground_truth_choice_prob = forager.choice_prob\n",
    "ground_truth_q_value = forager.q_value\n",
    "\n",
    "# Get the history\n",
    "print(ground_truth_params)\n",
    "\n",
    "choice_history = forager.get_choice_history()\n",
    "reward_history = forager.get_reward_history()\n",
    "\n",
    "# Plot the session results\n",
    "fig, axes = forager.plot_session(if_plot_latent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab523dd",
   "metadata": {},
   "source": [
    "# Parameter Recovery\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f3ff77",
   "metadata": {},
   "outputs": [],
   "source": [
    "forager.fit(\n",
    "    choice_history,\n",
    "    reward_history,\n",
    "    clamp_params={\"biasL\": 0},\n",
    "    DE_kwargs=dict(workers=4, disp=True, seed=np.random.default_rng(42)),\n",
    "    k_fold_cross_validation=None,\n",
    ")\n",
    "\n",
    "fitting_result = forager.fitting_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d98d095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check fitted parameters\n",
    "fit_names = fitting_result.fit_settings[\"fit_names\"]\n",
    "ground_truth = [num for name, num in ground_truth_params.items() if name in fit_names]\n",
    "print(f\"Num of trials: {len(choice_history)}\")\n",
    "print(f\"Fitted parameters: {fit_names}\")\n",
    "print(f'Ground truth: {[f\"{num:.4f}\" for num in ground_truth]}')\n",
    "print(f'Fitted:       {[f\"{num:.4f}\" for num in fitting_result.x]}')\n",
    "print(f\"Likelihood-Per-Trial: {fitting_result.LPT}\")\n",
    "print(f\"Prediction accuracy full dataset: {fitting_result.prediction_accuracy}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8687ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the fitted session results\n",
    "fig_fitting, axes = forager.plot_fitted_session(if_plot_latent=True)\n",
    "\n",
    "# Overlay the ground truth Q-values for comparison\n",
    "axes[0].plot(ground_truth_q_value[0], lw=1, color=\"red\", ls=\"-\", label=\"actual_Q(L)\")\n",
    "axes[0].plot(ground_truth_q_value[1], lw=1, color=\"blue\", ls=\"-\", label=\"actual_Q(R)\")\n",
    "axes[0].legend(fontsize=6, loc=\"upper left\", bbox_to_anchor=(0.6, 1.3), ncol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afe1c9b",
   "metadata": {},
   "source": [
    "# Model: Noisy win-stay-lose-shift\n",
    "\n",
    "\n",
    "The win-stay-lose-shift model is one of the simplest models that adapts its behavior according to feedback. Consistent with the name, the model repeats rewarded actions and switches away from unrewarded actions.\n",
    "\n",
    "\n",
    " In the noisy version of the model, the win-stay-lose-shift rule is applied probabilistically, such that the model applies the win-stay-lose-shift rule with probability $1−\\epsilon$, and chooses randomly with probability $\\epsilon$. In the two-bandit case, the probability of choosing option k is\n",
    "Let \\( c_t \\in \\{1, 2\\} \\) be the choice at trial \\( t \\), and \\( r_t \\in \\{0, 1\\} \\) the reward at trial \\( t \\). Then:\n",
    "\n",
    "$$\n",
    "p_k^t = \n",
    "\\begin{cases}\n",
    "1 - \\frac{\\epsilon}{2} & \\text{if } (c_{t-1} = k \\land r_{t-1} = 1) \\text{ or } (c_{t-1} \\neq  k \\land r_{t-1} = 0) \\\\\n",
    "\\frac{\\epsilon}{2} & \\text{if } (c_{t-1} \\not = k \\land r_{t-1} = 1) \\text{ or } (c_{t-1} = k \\land r_{t-1} = 0)\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Although more complex to implement, this model still only has one free parameter, the overall level of randomness, \\( \\theta_2 = \\epsilon \\)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9d0555",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b60647f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #         \"Win-Stay-Lose-Shift\": dict(\n",
    "#             description=\"The vanilla Win-stay-lose-shift model\",\n",
    "#             agent_class=\"ForagerLossCounting\",\n",
    "#             agent_kwargs=dict(\n",
    "#                 win_stay_lose_switch=True,\n",
    "#                 choice_kernel=\"none\",\n",
    "#             ),\n",
    "\n",
    "\n",
    "forager = ForagerLossCounting(win_stay_lose_switch=True, choice_kernel='none')\n",
    "\n",
    "# Initialize the model\n",
    "# forager = ForagerCollection().get_preset_forager(\"Hattori2019\", seed=42)\n",
    "forager.set_params(biasL = 0.3)\n",
    "\n",
    "# Create the task environment\n",
    "# task = CoupledBlockTask(reward_baiting=True, num_trials=1000, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b5ea51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "forager.perform(task)\n",
    "\n",
    "# Capture the results\n",
    "ground_truth_params = forager.params.model_dump()\n",
    "print(ground_truth_params)\n",
    "\n",
    "ground_truth_loss_count = forager.loss_count\n",
    "ground_truth_choice_prob = forager.choice_prob\n",
    "\n",
    "# Get the history\n",
    "choice_history = forager.get_choice_history()\n",
    "reward_history = forager.get_reward_history()\n",
    "\n",
    "# Plot the session results\n",
    "fig, axes = forager.plot_session(if_plot_latent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e3b5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to recover parameters\n",
    "forager.fit(\n",
    "    choice_history,\n",
    "    reward_history,\n",
    "    # fit_bounds_override={\"softmax_inverse_temperature\": [0, 100]},\n",
    "    DE_kwargs=dict(workers=4, disp=True, seed=np.random.default_rng(42)),\n",
    "    k_fold_cross_validation=None,\n",
    ")\n",
    "\n",
    "fitting_result = forager.fitting_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4b091a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check fitted parameters\n",
    "fit_names = fitting_result.fit_settings[\"fit_names\"]\n",
    "print(fit_names)\n",
    "ground_truth = [num for name, num in ground_truth_params.items() if name in fit_names]\n",
    "print(f\"Num of trials: {len(choice_history)}\")\n",
    "print(f\"Fitted parameters: {fit_names}\")\n",
    "print(f'Ground truth: {[f\"{num:.4f}\" for num in ground_truth]}')\n",
    "print(f'Fitted:       {[f\"{num:.4f}\" for num in fitting_result.x]}')\n",
    "print(f\"Likelihood-Per-Trial: {fitting_result.LPT}\")\n",
    "print(f\"Prediction accuracy full dataset: {fitting_result.prediction_accuracy}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d545785e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the fitted session results\n",
    "fig_fitting, axes = forager.plot_fitted_session(if_plot_latent=True)\n",
    "\n",
    "# Overlay the ground truth Q-values for comparison\n",
    "axes[0].plot(ground_truth_q_value[0], lw=1, color=\"red\", ls=\"-\", label=\"actual_Q(L)\")\n",
    "axes[0].plot(ground_truth_q_value[1], lw=1, color=\"blue\", ls=\"-\", label=\"actual_Q(R)\")\n",
    "axes[0].legend(fontsize=6, loc=\"upper left\", bbox_to_anchor=(0.6, 1.3), ncol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66427fb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f3f6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = 1000\n",
    "\n",
    "forager = 'LossCounting'\n",
    "para_names = ['loss_count_threshold_mean','loss_count_threshold_std']\n",
    "para_bounds = [[0,0],[30,10]]\n",
    "\n",
    "# Para recovery\n",
    "true_paras = generate_true_paras([[0,0],[30,0]], n_models = 30, method = 'random_uniform')\n",
    "\n",
    "fit_para_recovery(forager = forager, \n",
    "                  para_names = para_names, para_bounds = para_bounds, \n",
    "                  true_paras = true_paras, n_trials = n_trials, \n",
    "                  fit_method = 'DE', pool = pool);    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e108f47",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9cf23cba",
   "metadata": {},
   "source": [
    "# Testing on Han's model comparison code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1de592",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_worker = int(mp.cpu_count()/2)\n",
    "pool = mp.Pool(processes = n_worker)\n",
    "    \n",
    "from utils_model_recovery import *\n",
    "\n",
    "# n_trials = 400 # \n",
    "\n",
    "# forager = 'LossCounting'\n",
    "# para_names = ['loss_count_threshold_mean','loss_count_threshold_std']\n",
    "# para_bounds = [[0,0],[30,10]]\n",
    "\n",
    "# # Para recovery\n",
    "# true_paras = generate_true_paras(para_bounds=[[0,0],[30,0]], n_models = 30, method = 'random_uniform')\n",
    "\n",
    "# fit_para_recovery(forager = forager, \n",
    "#                   para_names = para_names, para_bounds = para_bounds, \n",
    "#                   true_paras = true_paras, n_trials = n_trials, \n",
    "#                   fit_method = 'DE', pool = pool);    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a26a164",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = 1000\n",
    "forager = 'LossCounting'\n",
    "para_names = ['loss_count_threshold_mean','loss_count_threshold_std']\n",
    "para_bounds = [[0,0],[50,10]]\n",
    "\n",
    "true_para = [10,4]\n",
    "\n",
    "# LL_surface\n",
    "compute_LL_surface(forager, para_names, para_bounds,\n",
    "                   true_para = true_para, n_trials = n_trials,\n",
    "                   fit_method = 'L-BFGS-B', n_x0s = 8, pool = '');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1185e7",
   "metadata": {},
   "source": [
    "# demonstrate parameter recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c3ddde",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = 1000\n",
    "\n",
    "forager = 'LossCounting'\n",
    "para_names = ['loss_count_threshold_mean','loss_count_threshold_std']\n",
    "para_bounds = [[0,0],[50,10]]\n",
    "\n",
    "# Para recovery\n",
    "true_paras = generate_true_paras([[0,0],[30,5]], n_models = [5,5], method = 'linspace')\n",
    "\n",
    "fit_para_recovery(forager = forager, \n",
    "                  para_names = para_names, para_bounds = para_bounds, \n",
    "                  true_paras = true_paras, n_trials = n_trials, \n",
    "                  fit_method = 'DE', pool = pool);    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ae9e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = 1000\n",
    "\n",
    "forager = 'LNP_softmax'\n",
    "para_names = ['tau1','softmax_temperature']\n",
    "para_scales = ['linear','log']\n",
    "para_bounds = [[1e-3,1e-2],[50,15]]\n",
    "\n",
    "n_models = 50\n",
    "true_paras = np.vstack((10**np.random.uniform(0, np.log10(30), size = n_models),\n",
    "                        1/np.random.exponential(10, size = n_models))) # Inspired by Wilson 2019. I found beta ~ Exp(10) would be better\n",
    "\n",
    "true_paras, fitted_para = fit_para_recovery(forager, \n",
    "              para_names, para_bounds, true_paras, n_trials = n_trials, \n",
    "              para_scales = para_scales,\n",
    "              fit_method = 'DE', pool = pool);  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc35cd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = 1000\n",
    "\n",
    "forager = 'LNP_softmax'\n",
    "para_names = ['tau1','softmax_temperature']\n",
    "para_scales = ['linear','log']\n",
    "para_bounds = [[1e-3,1e-2],[50,15]]\n",
    "\n",
    "compute_LL_surface(forager, para_names, para_bounds, para_scales = para_scales,\n",
    "                   true_para = [20, .1], n_trials = n_trials, \n",
    "                   fit_method = 'L-BFGS-B', n_x0s = 8, pool = '')\n",
    "\n",
    "\n",
    "n_trials = 1000\n",
    "\n",
    "forager = 'LNP_softmax'\n",
    "para_names = ['tau1','softmax_temperature']\n",
    "para_scales = ['linear','log']\n",
    "para_bounds = [[1e-3,1e-2],[50,15]]\n",
    "\n",
    "compute_LL_surface(forager, para_names, para_bounds, para_scales = para_scales,\n",
    "                   true_para = [20, .1], n_trials = n_trials, \n",
    "                   fit_method = 'DE', n_x0s = 8, pool = pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f2ad73",
   "metadata": {},
   "source": [
    "# Demonstrate model recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b9894a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ce92f02",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[Wilson, Collins, Elife, 2019] ​\n",
    "Ten simple rules for the computational modeling of behavioral data​\n",
    "\n",
    "\n",
    "> **Reference**  \n",
    "> Palminteri, S., Wyart, V., & Koechlin, É. (2017). The importance of falsification in computational cognitive modeling. *Trends in Cognitive Sciences*, 21(6), 425-433."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3a5153",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swdb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
