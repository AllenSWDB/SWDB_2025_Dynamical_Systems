{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5847ae3b",
   "metadata": {},
   "source": [
    "<!-- # made some changes change in environment:\n",
    "add statsmodels, seaborn  to pip install \n",
    "1. [x] Introduction about the dynamic foraging task (Jeremiah just has half an hour lecture so maybe we need some explanation or some figures to explain the mechanism) [Shuchen]\n",
    "2. [x] Introduction about generative model, [x] R-W rule [ ] winstayloseshift[Shuchen], and how this code simulates data\n",
    "3. [x] Why and how to perform model simulation [Shuchen]\n",
    "4. [x] How to perform model fitting and parameter recovery [Shuchen]\n",
    "5. [] make things more coherent, check what else would be needed\n",
    "6. [x] getting more parameter recovery plots, to make the process more intuitive?\n",
    "7. [x] I do not understand how the simple win-stay-lose-shift translate to Han's code\n",
    "    - I THINK the code could only implement a non-noisy WSLS model [Yusi]\n",
    "8. [x] Let's show a parameter recovery plot for RW1972 \n",
    "9. [x] And also a model comparison plot between a number of models\n",
    "10/. [x] clean up some code in the util function\n",
    "11. [x] add explanations of the code class [Yusi]\n",
    "12. [x] Do we nned to combine env./agent simulation code with the text explanation at the beginning? [Yusi]\n",
    "13. [x] add exercise problems [Yusi]\n",
    " -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31858956",
   "metadata": {},
   "source": [
    "<img src=\"./resources/cropped-SummerWorkshop_Header.png\">  \n",
    "\n",
    "<h1 align=\"center\">DAY 3 Workshop SWDB 2025 </h1> \n",
    "<h3 align=\"center\">Wednesday, August 27th, 2025</h3> \n",
    "<h3 align=\"center\">Fiting Models on Behavioral Data</h3> \n",
    "\n",
    "<!-- \n",
    "#  Objective \n",
    "-understand computational algorithms governing observed behavior\n",
    "\n",
    "-recover these algorithms by model fitting and model comparison\n",
    "\n",
    "-identify computational algorithms used by mice in the Dynamic Foraging task -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86eacdd",
   "metadata": {},
   "source": [
    "<!-- # Overview\n",
    "<img src=\"./resources/model_fit.png\" width=\"400\"> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15882b4",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "- Dynamic Foraging data exploration\n",
    "- Behavioral models\n",
    "    - Simulations\n",
    "    - Parameter recovery\n",
    "    - Model recovery\n",
    "- Model fitting of Dynamic Foraging data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50582647-35fe-45ec-a6ed-4e06eee1b58b",
   "metadata": {},
   "source": [
    "<!-- # Behavioral Data\n",
    "Behavioral data typically consist of choices, reaction times, eye movements, and sometimes neural signals. Let's load some behavioral data from the patch foraging task.  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f64365-8903-40b1-9244-a4d43b85163f",
   "metadata": {},
   "source": [
    "# The Dynamic Foraging Task\n",
    "\n",
    "- A mouse hears a \"go cue\" and chooses to **lick left** or **lick right**.\n",
    "- The **probability of receiving a water reward** for each action changes over time without the animal's knowledge.\n",
    "- The animal must learn and adapt to infer which action is currently more rewarding.\n",
    "- We want to know which strategy the animal is using to make choices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a080d924",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<img src=\"./resources/dyn_foraging_task_schematic.png\" alt=\"Foraging Task Schematic\" width=600>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39e4aa2",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28a53d5c-441e-437a-93c7-ba77c7dbc31c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from aind_analysis_arch_result_access.han_pipeline import get_session_table # need to remove this dependency later \n",
    "from pynwb import NWBHDF5IO\n",
    "from hdmf_zarr import NWBZarrIO\n",
    "from aind_behavior_gym.dynamic_foraging.task import UncoupledBlockTask\n",
    "import numpy as np\n",
    "from utils_model_recovery import *\n",
    "import multiprocessing as mp\n",
    "n_worker = int(mp.cpu_count() / 2)\n",
    "\n",
    "# Finding root directory \n",
    "import sys\n",
    "from os.path import join as pjoin\n",
    "import platform\n",
    "platstring = platform.platform()\n",
    "system = platform.system()\n",
    "if system == \"Darwin\":\n",
    "    # macOS\n",
    "    data_root = \"/Volumes/Brain2025/\"\n",
    "elif system == \"Windows\":\n",
    "    # Windows (replace with the drive letter of USB drive)\n",
    "    data_root = \"E:/\"\n",
    "elif \"amzn\" in platstring:\n",
    "    # then on CodeOcean\n",
    "    data_root = \"/data/\"\n",
    "else:\n",
    "    # then your own linux platform\n",
    "    # EDIT location where you mounted hard drive\n",
    "    data_root = \"/media/$USERNAME/Brain2025/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8aa5afb-7c9d-4b17-b108-557feaf1f827",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "PathNotFoundError",
     "evalue": "nothing found at path ''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPathNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m example_behavior_file \u001b[38;5;241m=\u001b[39m data_root \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdynamic_foraging-behavior/behavior_729678_2025-01-10_09-24-20_processed_2025-02-22_15-57-11/nwb/behavior_729678_2025-01-10_09-24-20.nwb\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m io\u001b[38;5;241m=\u001b[39m\u001b[43mNWBZarrIO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample_behavior_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m nwb_behavior_data \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/hdmf/utils.py:578\u001b[0m, in \u001b[0;36mdocval.<locals>.dec.<locals>.func_call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc_call\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    577\u001b[0m     pargs \u001b[38;5;241m=\u001b[39m _check_args(args, kwargs)\n\u001b[0;32m--> 578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/hdmf_zarr/nwb.py:47\u001b[0m, in \u001b[0;36mNWBZarrIO.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_namespaces:\n\u001b[1;32m     46\u001b[0m     tm \u001b[38;5;241m=\u001b[39m get_type_map()\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_namespaces\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m     manager \u001b[38;5;241m=\u001b[39m BuildManager(tm)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/hdmf/utils.py:578\u001b[0m, in \u001b[0;36mdocval.<locals>.dec.<locals>.func_call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc_call\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    577\u001b[0m     pargs \u001b[38;5;241m=\u001b[39m _check_args(args, kwargs)\n\u001b[0;32m--> 578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/hdmf_zarr/backend.py:272\u001b[0m, in \u001b[0;36mZarrIO.load_namespaces\u001b[0;34m(cls, namespace_catalog, path, storage_options, namespaces)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;124;03mLoad cached namespaces from a file.\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;66;03m# TODO: how to use storage_options here?\u001b[39;00m\n\u001b[0;32m--> 272\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[43mzarr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m SPEC_LOC_ATTR \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m f\u001b[38;5;241m.\u001b[39mattrs:\n\u001b[1;32m    274\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo cached namespaces found in \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m path\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/zarr/convenience.py:137\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(store, mode, zarr_version, path, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m open_group(_store, mode\u001b[38;5;241m=\u001b[39mmode, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PathNotFoundError(path)\n",
      "\u001b[0;31mPathNotFoundError\u001b[0m: nothing found at path ''"
     ]
    }
   ],
   "source": [
    "example_behavior_file = data_root + 'dynamic-foraging-behavior/behavior_729678_2025-01-10_09-24-20_processed_2025-02-22_15-57-11/nwb/behavior_729678_2025-01-10_09-24-20.nwb'\n",
    "io=NWBZarrIO(example_behavior_file, \"r\")\n",
    "nwb_behavior_data = io.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690f09fb-d93f-40a1-a122-25876a8a201e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print out the meta-data\n",
    "# behavior_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62c941c-eab3-452e-a6b9-d005af35b4e0",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Explanation of column names</summary>\n",
    "\n",
    "**nwb_behavior_data.acquisition**\n",
    "- `left_reward_delivery_time` : The reward delivery time of the left lick spout. \n",
    "- `right_reward_delivery_time` : The reward delivery time of the right lick spout. \n",
    "- `left_lick_time` : The time of the left lick.\n",
    "- `right_lick_time` : The time of the right lick.\n",
    "\n",
    "**nwb_behavior_data.intervals['trials']**\n",
    "\n",
    "Behavior events\n",
    "- `animal_response` : \"The response of the animal. 0, left choice; 1, right choice; 2, no response\"\n",
    "- `rewarded_historyL` : \"The reward history of left lick port\"\n",
    "- `rewarded_historyR` : \"The reward history of right lick port\"\n",
    "- `delay_start_time` : \"The delay start time\"\n",
    "- `goCue_start_time` : \"The go cue start time\"\n",
    "- `reward_outcome_time` : \"The reward outcome time (reward/no reward/no response) Note: This is in fact time when choice is registered.\n",
    "- `start_time` : \"Trial start time\"\n",
    "- `end_time` : \"Trial end time\"\n",
    "\n",
    "Training parameters - behavior structure\n",
    "- `bait_left` : \"Whether the current left lickport has a bait or not\"\n",
    "- `bait_right` : \"Whether the current right lickport has a bait or not\"\n",
    "- `base_reward_probability_sum` : \"The summation of left and right reward probability\"\n",
    "- `reward_probabilityL` : \"The reward probability of left lick port\"\n",
    "- `reward_probabilityR` : \"The reward probability of right lick port\"\n",
    "- `reward_random_number_left` : \"The random number used to determine the reward of left lick port\"\n",
    "- `reward_random_number_right` : \"The random number used to determine the reward of right lick port\"\n",
    "- `left_valve_open_time` : \"The left valve open time\"\n",
    "- `right_valve_open_time` : \"The right valve open time\"\n",
    "\n",
    "Training parameters - block\n",
    "- `block_beta` : \"The beta of exponential distribution to generate the block length\"\n",
    "- `block_min` : \"The minimum length allowed for each block\"\n",
    "- `block_max` : \"The maxmum length allowed for each block\"\n",
    "- `min_reward_each_block` : \"The minimum reward allowed for each block\"\n",
    "\n",
    "Training parameters - delay duration\n",
    "- `delay_beta` : \"The beta of exponential distribution to generate the delay duration(s)\"\n",
    "- `delay_min` : \"The minimum duration(s) allowed for each delay\"\n",
    "- `delay_max` : \"The maxmum duration(s) allowed for each delay\"\n",
    "- `delay_duration` : \"The expected time duration between delay start and go cue start\"\n",
    "\n",
    "Training parameters - ITI duration\n",
    "- `ITI_beta` : \"The beta of exponential distribution to generate the ITI duration(s)\"\n",
    "- `ITI_min` : \"The minimum duration(s) allowed for each ITI\"\n",
    "- `ITI_max` : \"The maxmum duration(s) allowed for each ITI\"\n",
    "- `ITI_duration` : \"The expected time duration between trial start and ITI start\"\n",
    "\n",
    "Training parameters - response duration\n",
    "- `response_duration` : \"The maximum time that the animal must make a choce in order to get a reward\"\n",
    "\n",
    "Training parameters - reward consumption duration\n",
    "- `reward_consumption_duration` : \"The duration for the animal to consume the reward\"\n",
    "\n",
    "Training parameters - reward delay\n",
    "- `reward_delay` : \"The delay between choice and reward delivery\"\n",
    "\n",
    "Training parameters - auto water\n",
    "- `auto_waterL` : \"Autowater given at Left\"\n",
    "- `auto_waterR` : \"Autowater given at Right\"\n",
    "\n",
    "Training parameters - optogenetics\n",
    "- `laser_on_trial` : \"Trials with laser stimulation\"\n",
    "- `laser_wavelength` : \"The wavelength of laser or LED\"\n",
    "- `laser_location` : \"The target brain areas\"\n",
    "- `laser_1_power` : \"The laser power of the laser 1(mw)\"\n",
    "- `laser_2_power` : \"The laser power of the laser 2(mw)\"\n",
    "- `laser_on_probability` : \"The laser on probability\"\n",
    "- `laser_duration` : \"The laser duration\"\n",
    "- `laser_condition` : \"The laser on is conditioned on LaserCondition\"\n",
    "- `laser_condition_probability` : \"The laser on is conditioned on LaserCondition with a probability LaserConditionPro\"\n",
    "- `laser_start` : \"Laser start is aligned to an event\"\n",
    "- `laser_start_offset` : \"Laser start is aligned to an event with an offset\"\n",
    "- `laser_end` : \"Laser end is aligned to an event\"\n",
    "- `laser_end_offset` : \"Laser end is aligned to an event with an offset\"\n",
    "- `laser_protocol` : \"The laser waveform\"\n",
    "- `laser_frequency` : \"The laser waveform frequency\"\n",
    "- `laser_rampingdown` : \"The ramping down time of the laser\"\n",
    "- `laser_pulse_duration` : \"The pulse duration for Pulse protocol\"\n",
    "- `session_wide_control` : \"Control the optogenetics session wide (e.g. only turn on opto in half of the session)\"\n",
    "- `fraction_of_session` : \"Turn on/off opto in a fraction of the session (related to session_wide_control)\"\n",
    "- `session_start_with` : \"The session start with opto on or off (related to session_wide_control)\"\n",
    "- `session_alternation` : \"Turn on/off opto in every other session (related to session_wide_control)\"\n",
    "- `minimum_opto_interval` : \"Minimum interval between two optogenetics trials (number of trials)\"\n",
    "\n",
    "Training parameters - auto training parameters\n",
    "- `auto_train_engaged` : \"Whether the auto training is engaged\"\n",
    "- `auto_train_curriculum_name` : \"The name of the auto training curriculum\"\n",
    "- `auto_train_curriculum_version` : \"The version of the auto training curriculum\"\n",
    "- `auto_train_curriculum_schema_version` : \"The schema version of the auto training curriculum\"\n",
    "- `auto_train_stage` : \"The current stage of auto training\"\n",
    "- `auto_train_stage_overridden` : \"Whether the auto training stage is overridden\"\n",
    "    \n",
    "Training parameters - lickspout position\n",
    "- `lickspout_position_x` : \"x position (um) of the lickspout position (left-right)\"\n",
    "- `lickspout_position_y` : \"y position (um) of the lickspout position (forward-backward)\"\n",
    "- `lickspout_position_z` : \"z position (um) of the lickspout position (up-down)\"\n",
    "\n",
    "Training parameters - reward size\n",
    "- `reward_size_left` : \"Left reward size (uL)\"\n",
    "- `reward_size_right` : \"Right reward size (uL)\"\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13160d4e-0461-465d-88f6-6f721c2f149a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Behavioral description\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c7de0e-f559-4898-b698-1d511d1818f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trials = nwb_behavior_data.intervals['trials'][:]\n",
    "choice_history = trials['animal_response'].to_numpy()\n",
    "choice_history[choice_history == 2] = np.nan\n",
    "reward_history = (trials['rewarded_historyL'] | trials['rewarded_historyR']).astype(float).to_numpy()\n",
    "print(choice_history[:5])\n",
    "print(reward_history[:5])\n",
    "print('Number of trials: {}'.format(len(choice_history)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d14870-b5b1-4f8a-a81a-cbeb29a03894",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ignored = np.isnan(choice_history) \n",
    "choice_history_real_data = choice_history[~ignored]\n",
    "reward_history_real_data = reward_history[~ignored]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1b02a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(choice_history_real_data[:5]) # 0: left, 1: right\n",
    "print(reward_history_real_data[:5]) # 0: unrewarded, 1: rewarded\n",
    "print('Number of trials: {}'.format(len(choice_history_real_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5951960e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# variables needed for this plot: choice_history_real_data, reward_history_real_data\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 3), dpi=200)\n",
    "\n",
    "rewarded_trials = reward_history_real_data == 1\n",
    "unrewarded_trials = reward_history_real_data == 0\n",
    "xx = np.nonzero(rewarded_trials)[0] + 1\n",
    "yy_temp = choice_history_real_data[rewarded_trials]\n",
    "yy_right = yy_temp[yy_temp > 0.5] + 0.05\n",
    "xx_right = xx[yy_temp > 0.5]\n",
    "yy_left = yy_temp[yy_temp < 0.5] - 0.05\n",
    "xx_left = xx[yy_temp < 0.5]\n",
    "\n",
    "# Rewarded choices\n",
    "ax.vlines(xx_right, yy_right, yy_right + 0.1, alpha=1, linewidth=1, color=\"black\", label=\"Rewarded choices\")\n",
    "ax.vlines(xx_left, yy_left - 0.1, yy_left, alpha=1, linewidth=1, color=\"black\")\n",
    "\n",
    "unrewarded_trials = reward_history == 0\n",
    "xx = np.nonzero(unrewarded_trials)[0] + 1\n",
    "yy_temp = choice_history[unrewarded_trials]\n",
    "yy_right = yy_temp[yy_temp > 0.5]\n",
    "xx_right = xx[yy_temp > 0.5]\n",
    "yy_left = yy_temp[yy_temp < 0.5]\n",
    "xx_left = xx[yy_temp < 0.5]\n",
    "\n",
    "# Unrewarded choices\n",
    "ax.vlines(xx_right, yy_right + 0.05, yy_right + 0.1, alpha=1, linewidth=1, color=\"gray\", label=\"Unrewarded choices\")\n",
    "ax.vlines(xx_left, yy_left - 0.1, yy_left - 0.05, alpha=1, linewidth=1, color=\"gray\")\n",
    "\n",
    "# Set up axis labels and limits\n",
    "ax.set_xlabel('Trial number')\n",
    "ax.set_ylim(-0.2, 1.2)\n",
    "ax.set_yticks([0, 1])\n",
    "ax.set_yticklabels(['Left', 'Right'])\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.title('Choice (0=Left, 1=Right)')\n",
    "plt.tight_layout()\n",
    "plt.show()# plotting of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99072b08",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "**Question**: How to reveal what the animals are thinking? i.e. read the mouse's mind \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad5575c",
   "metadata": {},
   "source": [
    "<div align=\"left\"><img src=\"resources/mouse_left_right.png\" width=\"200\"></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01036aec",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Answer</summary>\n",
    "\n",
    "through behavioral model fitting\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7740aa",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Behavioral Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d3ae98",
   "metadata": {},
   "source": [
    "Model fitting is a powerful tool for making cognitive theories explicit, testable, and quantitatively precise. It typically involves:\n",
    "\n",
    "1. **Simulation** – to understand what the model predicts.\n",
    "2. **Parameter estimation** – to fit the model to data.\n",
    "3. **Model comparison** – to evaluate competing hypotheses.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639dabf1",
   "metadata": {},
   "source": [
    "\n",
    "| Model Name                | Description                                                                 | Parameters $\\theta$                                                                                  |\n",
    "|---------------------------|-----------------------------------------------------------------------------|----------------------------------------------------------------------------------------------|\n",
    "| Rescorla-Wagner $\\epsilon$-Greedy     | Value update via RPE and $\\epsilon$-greedy action selection         | $\\alpha$ (learning rate), $\\epsilon$ (exploration rate)  |\n",
    "| Win-Stay Lose-Shift (biased) | Repeat rewarded action, switch after loss, with bias                        | $b_L$ (bias towards left choice)                                                                                    |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e436de",
   "metadata": {},
   "source": [
    "\n",
    "## Model 1: Rescorla-Wagner Epsilon-Greedy\n",
    "\n",
    "This model captures how action values are incrementally adjusted over time. The expected value of each action is updated based on the discrepancy between received and expected outcomes. Specifically, the value of option $L$ on trial $t$, denoted $Q^L_t$, is updated according to:\n",
    "\n",
    "$$\n",
    "Q^L_{t+1} = Q^L_t + \\alpha c^L_t (r_t - Q^L_t)\n",
    "$$\n",
    "\n",
    "To translate learned values into choices, one way is to use $\\epsilon$ greedy stratefy, balancing exploration versus exploitation. \n",
    "$$\n",
    "c_t =\n",
    "\\begin{cases}\n",
    "\\text{random action}, & \\text{with probability } \\epsilon \\\\\n",
    "\\arg\\max_{k \\in \\{L,R\\}} Q^k, & \\text{with probability } 1 - \\epsilon\n",
    "\\end{cases}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cdf862",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> Denotations </summary>\n",
    "\n",
    "- $c^L_t \\in \\{0,1\\}$ is an indicator function of choosing left on trial $t$ \n",
    "- $r_t \\in \\{0,1\\}$ is the reward on trial $t$,\n",
    "- $\\alpha \\in [0, 1]$ is the **learning rate**, controlling how strongly the prediction error $(r_t - Q^L_t)$ updates the value,\n",
    "- $Q^L_0$ is typically initialized to zero or treated as a free parameter.\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b3b5f8",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Model 2: Biased Win-Stay Lose-Shift \n",
    "\n",
    "If the agent received a reward for choosing an action on the previous trial (a win), it repeats that action; if the agent did not receive a reward (a loss), it switches to the alternative action on the next trial. To take account of the agent's preferrance of one decision over another, often a bias parameter ($b_L$) is included.\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "**List all outcomes:**\n",
    "\n",
    "| Previous Choice | Previous Reward | Probability of Choosing L at $t$ |\n",
    "|-----------------|----------------|:--------------------------------:|\n",
    "| Left  | Win  | $ (b_L + 1) / (b_L + 1)$ = 1 |\n",
    "| Left  | Loss | $(b_L) / (b_L + 1)$           |\n",
    "| Right | Win  | $(b_L) / (b_L + 1)$           |\n",
    "| Right | Loss | $(b_L + 1) / (b_L + 1)$ = 1   |\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323e6334",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary >Model 2 Variant: Loss Counting with Probabilistic Threshold </summary>\n",
    "\n",
    "\n",
    "\n",
    "A more general variant of the win-stay lose-shift model is loss counting with a probabilistic threshold. The model switches arms when the count of loss (no reward) $\\ell_t$ from one arm exceed a (stochastic) loss threshold $\\tau_t$ with mean $\\mu_{LC}$ and variance $\\sigma_{LC}$. \n",
    "\n",
    "Action switches when \n",
    "$$\n",
    "l_t \\ge \\tau_t\n",
    "$$\n",
    "\n",
    "Redraw threshold when switching\n",
    "$$\n",
    "\\tau_{t+1} \\sim \\mathcal{N}(\\mu_{LC},\\sigma_{LC}^2) \n",
    "$$\n",
    "\n",
    "\n",
    "The model is parameterized by:\n",
    "$\n",
    "\\boldsymbol{\\theta} = (\\mu_{LC},\\sigma_{LC})\n",
    "$\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fcebe6-18c0-48bc-9ca6-c15e006cf1a5",
   "metadata": {},
   "source": [
    "## Step 1: simulation:\n",
    "\n",
    "- Simulate surrogate data from models and task variants\n",
    "- Compare behaviors across models and parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3300285",
   "metadata": {},
   "source": [
    "### Environment initialization\n",
    "First we want to initialize a world environment which will interact with the agent to give feedback, including reward/force switch etc. This is like the rules for the task. The `UncoupledBlockTask` is a wrapped environment where reward probabilities for each action (e.g., left or right) change **independently** across blocks of trials."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f754a93",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b> Class UncoupledBlockTask explained </b></summary>\n",
    "\n",
    "**Overview**\n",
    "\n",
    "- The `UncoupledBlockTask` class implements a dynamic foraging environment where reward probabilities for left and right choices change **independently** across blocks of trials.\n",
    "\n",
    "**Initialization Arguments**\n",
    "- `reward_baiting`: `bool = False`\n",
    "  - Whether rewards persist until collected\n",
    "  - `True`: Uncollected rewards remain available\n",
    "  - `False`: Rewards are generated fresh each trial\n",
    "\n",
    "- `allow_ignore`: `bool = False`\n",
    "  - Whether agent can skip trials\n",
    "  - `True`: Adds \"ignore\" as third action option\n",
    "  - `False`: Agent must choose left or right\n",
    "\n",
    "- `num_arms`: `int = 2`\n",
    "  - Number of choice options (typically 2 for left/right)\n",
    "\n",
    "- `num_trials`: `int = 1000`\n",
    "  - Total number of trials in the session\n",
    "\n",
    "- `seed`: `int = None`\n",
    "  - Random seed for reproducibility\n",
    "\n",
    "- `rwd_prob_array`: `List[float] = [0.1, 0.5, 0.9]`\n",
    "  - Available reward probabilities for block assignment\n",
    "  - Each new block randomly selects from this array\n",
    "\n",
    "- `block_min`: `int = 20`\n",
    "  - Minimum block length in trials\n",
    "\n",
    "- `block_max`: `int = 35`\n",
    "  - Maximum block length in trials\n",
    "\n",
    "- `persev_add`: `bool = True`\n",
    "  - Enable anti-perseveration mechanism\n",
    "\n",
    "- `perseverative_limit`: `int = 4`\n",
    "  - Number of consecutive choices on min-prob side to trigger anti-persev\n",
    "\n",
    "- `max_blocc_tally`: `int = 4`\n",
    "  - Maximum consecutive blocks one side can be better before forced balancing\n",
    "\n",
    "**Key Methods**\n",
    "\n",
    "- `reset()`: Initialize new session with fresh block schedule\n",
    "- `step(action)`: Execute one trial and return (observation, reward, done, info)\n",
    "- `generate_new_trial()`: Create reward probabilities for next trial\n",
    "- `get_choice_history()`, `get_reward_history()`: Extract behavioral data\n",
    "- `get_p_reward()`: Get reward probability matrix for analysis\n",
    "- `plot_reward_schedule()`: Visualize block structure and choice patterns\n",
    "\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abb503a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "task = UncoupledBlockTask(reward_baiting=False, num_trials=100, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e632b7",
   "metadata": {},
   "source": [
    "### Agent initialization\n",
    "\n",
    "Next we want to initialize an agent which will interact with the task environment following its internal algorithm such as Q-learning. `ForagerQLearning` is a sophisticated reinforcement learning agent that implements Q-learning for dynamic foraging tasks. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03faae1a",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b> class ForagerQLearning explained </b></summary>\n",
    "\n",
    "**Overview:**\n",
    "\n",
    "The `ForagerQLearning` class is a flexible reinforcement learning agent that incrementally updates action-value estimates (Q-values) based on reward history, supports different learning rates/forget rates for rewarded versus unrewarded outcomes, and can incorporate choice history effects through choice kernels. \n",
    "\n",
    "**Initialization configs:**\n",
    "- `number_of_learning_rate`: `Literal[1, 2] = 2`\n",
    "  - Controls learning rate structure\n",
    "  - `1`: Single learning rate for all outcomes\n",
    "  - `2`: Separate rates for rewarded (`learn_rate_rew`) and unrewarded (`learn_rate_unrew`) outcomes\n",
    "\n",
    "- `number_of_forget_rate`: `Literal[0, 1] = 1`\n",
    "  - Controls forgetting mechanism\n",
    "  - `0`: No forgetting of unchosen options\n",
    "  - `1`: Includes `forget_rate_unchosen` parameter\n",
    "\n",
    "- `choice_kernel`: `Literal[\"none\", \"one_step\", \"full\"] = \"none\"`\n",
    "  - Choice history influence on decisions\n",
    "  - `\"none\"`: No choice history effects\n",
    "  - `\"one_step\"`: Only immediate previous choice (Bari2019 style)\n",
    "  - `\"full\"`: Exponentially weighted choice history with learnable parameters\n",
    "\n",
    "- `action_selection`: `Literal[\"softmax\", \"epsilon-greedy\"] = \"softmax\"`\n",
    "  - Decision strategy\n",
    "  - `\"softmax\"`: Probabilistic selection with inverse temperature parameter\n",
    "  - `\"epsilon-greedy\"`: Exploration with fixed epsilon probability\n",
    "\n",
    "- `params`: `dict = {}`\n",
    "  - Initial model parameters (auto-generated based on configuration)\n",
    "  - Learning rates: `learn_rate`, `learn_rate_rew`, `learn_rate_unrew` (0.0-1.0)\n",
    "  - Forget rates: `forget_rate_unchosen` (0.0-1.0)\n",
    "  - Bias: `biasL` (left side bias, -5.0 to 5.0)\n",
    "  - Action selection: `softmax_inverse_temperature` (0.0-100.0) or `epsilon` (0.0-1.0)\n",
    "  - Choice kernel: `choice_kernel_relative_weight`, `choice_kernel_step_size` (0.0-1.0)\n",
    "\n",
    "- `**kwargs`: Additional arguments passed to base class (e.g., `seed` for reproducibility)\n",
    "\n",
    "**Key Methods:**\n",
    "\n",
    "- `perform(task)`: Simulate agent behavior on a foraging task\n",
    "- `fit(choice_history, reward_history)`: Fit model parameters to behavioral data\n",
    "- `act(observation)`: Select action based on current Q-values\n",
    "- `learn(observation, action, reward, next_observation, done)`: Update Q-values after action\n",
    "- `plot_session()`: Visualize behavioral session and internal states\n",
    "- `get_choice_history()`, `get_reward_history()`: Extract behavioral data\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8981f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "forager_RW = ForagerQLearning(number_of_learning_rate=1,number_of_forget_rate=0,choice_kernel=\"none\",action_selection=\"epsilon-greedy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f325a7",
   "metadata": {},
   "source": [
    "**Exercise**: describe this Q-learning model\n",
    "\n",
    "1. How is the learning rate going to be updated? Same for all choices or differentiate between rewarded and unrewarded choices?\n",
    "2. What is the forget mechanism? \n",
    "3. Is previous choice going to influece the choice structure? \n",
    "4. What is the decision strategy? \n",
    "5. What are the parameters associated? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364e3471",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b> Answer </b></summary>\n",
    "\n",
    "1. Same learning rate for all choices\n",
    "2. No forgetting mechanism\n",
    "3. No choice kernel\n",
    "4. Epsilon-greedy decision strategy\n",
    "5. Parameters: `forager.params`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa722b2d",
   "metadata": {},
   "source": [
    "### Agent interacting with task environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aad3b28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Simulate the model\n",
    "forager_RW.perform(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05801b67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# obtain behaviorial data from the forager\n",
    "ground_truth_params = forager_RW.params.model_dump()\n",
    "ground_truth_choice_prob = forager_RW.choice_prob\n",
    "ground_truth_q_value = forager_RW.q_value\n",
    "print(\"Ground Truth Parameters:\", ground_truth_params)\n",
    "# Get the history\n",
    "choice_history = forager_RW.get_choice_history() # 0: Left, 1: Right\n",
    "reward_history = forager_RW.get_reward_history() # 0: Unrewarded, 1: Rewarded\n",
    "p_reward = forager_RW.task.get_p_reward() # row 0: Left, row 1: Right\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058af3e7",
   "metadata": {},
   "source": [
    "#### Visualization of reward probability of two arms in the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29299965",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reward probabilities fluctuate over trials between [0.1, 0.5, 0.9]\n",
    "print('Reward probability in two ports alternate independently between:', np.unique(p_reward))\n",
    "# visualization\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 3), dpi=200)\n",
    "ax.plot(task.get_p_reward()[0], label='P_left', color='red')\n",
    "ax.plot(task.get_p_reward()[1], label='P_right', color='blue')\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_xlabel('Trial')\n",
    "ax.set_ylabel('Rew. Prob.')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9842aa0e",
   "metadata": {},
   "source": [
    "#### Visualization choices by the simulated agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f17f7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(15, 3), dpi=200)\n",
    "\n",
    "rewarded_trials = reward_history == 1\n",
    "unrewarded_trials = reward_history == 0\n",
    "xx = np.nonzero(rewarded_trials)[0] + 1\n",
    "yy_temp = choice_history[rewarded_trials]\n",
    "yy_right = yy_temp[yy_temp > 0.5] + 0.05\n",
    "xx_right = xx[yy_temp > 0.5]\n",
    "yy_left = yy_temp[yy_temp < 0.5] - 0.05\n",
    "xx_left = xx[yy_temp < 0.5]\n",
    "\n",
    "# Rewarded choices\n",
    "ax.vlines(xx_right, yy_right, yy_right + 0.1, alpha=1, linewidth=1, color=\"black\", label=\"Rewarded choices\")\n",
    "ax.vlines(xx_left, yy_left - 0.1, yy_left, alpha=1, linewidth=1, color=\"black\")\n",
    "\n",
    "unrewarded_trials = reward_history == 0\n",
    "xx = np.nonzero(unrewarded_trials)[0] + 1\n",
    "yy_temp = choice_history[unrewarded_trials]\n",
    "yy_right = yy_temp[yy_temp > 0.5]\n",
    "xx_right = xx[yy_temp > 0.5]\n",
    "yy_left = yy_temp[yy_temp < 0.5]\n",
    "xx_left = xx[yy_temp < 0.5]\n",
    "\n",
    "# Unrewarded choices\n",
    "ax.vlines(xx_right, yy_right + 0.05, yy_right + 0.1, alpha=1, linewidth=1, color=\"gray\", label=\"Unrewarded choices\")\n",
    "ax.vlines(xx_left, yy_left - 0.1, yy_left - 0.05, alpha=1, linewidth=1, color=\"gray\")\n",
    "\n",
    "# Set up axis labels and limits\n",
    "ax.set_xlabel('Trial number')\n",
    "ax.set_ylim(-0.2, 1.2)\n",
    "ax.set_yticks([0, 1])\n",
    "ax.set_yticklabels(['Left', 'Right'])\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.title('Choice (0=Left, 1=Right)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba57a181",
   "metadata": {},
   "source": [
    "#### Visualization of choice statistics\n",
    "1. Base reward probabilities = p_right / (p_right + p_left)\n",
    "2. Choice = moving average of choice history, i.e. moving average of choosing right \n",
    "3. Q(L) (`ground_truth_q_value[0,:]`): ground truth q value for left choice\n",
    "4. Q(R) (`ground_truth_q_value[1,:]`): ground truth q value for right choice\n",
    "5. choice_prob(R/R+L) (`ground_truth_choice_prob[1] / ground_truth_choice_prob.sum(axis=0)`): choice probability normalized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1857970",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1, figsize=(15, 6), dpi=200)\n",
    "\n",
    "def moving_average(a, n=3):\n",
    "    \"\"\"Compute moving average of a list or array.\"\"\"\n",
    "    ret = np.nancumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[(n - 1):] / n\n",
    "\n",
    "n_trials = len(choice_history)\n",
    "smooth_factor = 5  \n",
    "\n",
    "ax = axs[0]\n",
    "ax.plot(np.arange(1,n_trials+1), p_reward[1, :] / (np.sum(p_reward, axis=0)),\n",
    "         color=\"gold\",label=\"Base rew. prob.\",lw=1.5,)\n",
    "y = moving_average(choice_history, n=smooth_factor)\n",
    "ax.plot(np.arange(0, len(y)) + int(smooth_factor / 2) + 1, y,\n",
    "        linewidth=1.5,color=\"black\",label=\"Choice (smooth = %g)\" % smooth_factor)\n",
    "ax = axs[1]\n",
    "ax.plot(np.arange(1,n_trials+2), ground_truth_q_value[0, :], lw=1, color=\"red\", ls=\"-\", label=\"Q(L)\")\n",
    "ax.plot(np.arange(1,n_trials+2), ground_truth_q_value[1, :], lw=1, color=\"blue\", ls=\"-\", label=\"Q(R)\")\n",
    "ax.plot(np.arange(1,n_trials+1), ground_truth_choice_prob[1] / np.sum(ground_truth_choice_prob, axis=0), lw=1, color=\"green\", ls=\"-\", label=\"Choice prob. (R/R+L)\")\n",
    "\n",
    "axs[0].legend()\n",
    "axs[1].legend()\n",
    "axs[1].set_xlabel('Trial number')\n",
    "axs[0].set_ylim(-0.2, 1.2)\n",
    "axs[1].set_ylim(-0.2, 1.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39de39b7",
   "metadata": {},
   "source": [
    "Question: why are Q values fluctuctuating?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7723f25",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b> Answer </b></summary>\n",
    "Unrewarded / unexpected choice outcome\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8187cf27",
   "metadata": {},
   "source": [
    "#### Visualization on one plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e700059c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axes = forager_RW.plot_session(if_plot_latent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704d686c",
   "metadata": {},
   "source": [
    "**Exercise**: \n",
    "Use another set of parameters and simulate more choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43eb10bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "forager_RW2 = ForagerQLearning(number_of_learning_rate=1,number_of_forget_rate=1,choice_kernel=\"none\",action_selection=\"epsilon-greedy\")\n",
    "print(forager_RW2.params)\n",
    "forager_RW2.perform(task)\n",
    "forager_RW2.plot_session(if_plot_latent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab523dd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step2: Parameter Recovery\n",
    "Parameter estimation refers to **inferring model parameters** that best explain observed behavior.\n",
    "Parameters are typically inferred using **maximum likelihood** or **Bayesian methods**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f6edb7-453b-4ad5-8b7f-ebcbe522cf5f",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> Maximum Likelihood explained </summary>\n",
    "\n",
    "The observed data  $d_{1:T}$ includes **choice history** $c_t \\in \\{\\text{L}, \\text{R}\\}$ and **reward history** $r_t \\in \\{0,1\\}$\n",
    "<!-- - **Reward history**: rewards associated with left and right actions:\n",
    "  - $r_{R,t}$: reward received when choosing right at time $t$\n",
    "  - $r_{L,t}$: reward received when choosing left at time $t$\n",
    " -->\n",
    "\n",
    "To fit a model to behavioral data, we estimate the parameters $\\boldsymbol{\\theta}_M$ that maximize the likelihood of the observed data under model $M$. This is typically done using **maximum likelihood estimation (MLE)**:\n",
    "\n",
    "$$\n",
    "\\hat{\\boldsymbol{\\theta}}^{\\text{MLE}}_M = \\arg\\max_{\\boldsymbol{\\theta}_M} \\log p(d_{1:T} \\mid \\boldsymbol{\\theta}_M, M) = \\arg\\max_{\\boldsymbol{\\theta}_M} \\sum_{t=1}^{T} \\log p(c_t \\mid d_{1:t-1}, \\boldsymbol{\\theta}_M, M)\n",
    "$$\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4514b40a",
   "metadata": {},
   "source": [
    "### Example: RW epsilon greedy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31be518-1115-4c09-831d-cb2943876ad9",
   "metadata": {},
   "source": [
    "Now we have access to the simulated agent's choices and environment feedback. How can we obtain the agent's parameter, which govern its choices from Q-learing model? We could utilize the `fit` function associated with `forager` class. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d674db",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b> forager.fit() explained </b></summary>\n",
    "\n",
    "**Main purpose** \n",
    "\n",
    "Finds the model parameters that maximize the likelihood of observing the actual choice and reward history. Computes model evaluation metrics (AIC, BIC, prediction accuracy). Optionally cross-validation.\n",
    "\n",
    "**Inputs** \n",
    "- `fit_choice_history` / `fit_reward_history`: Behavioral data to fit\n",
    "- `clamp_params`: Fix specific parameters (e.g., `{\"biasL\": 0}`)\n",
    "- `fit_bounds_override`: Custom parameter bounds for fitting\n",
    "- `k_fold_cross_validation`: Number of CV folds (None = no CV)\n",
    "- `DE_kwargs`: Optimization settings (workers, seed, etc.)\n",
    "\n",
    "**Outputs**\n",
    "- `fitting_result`: Best parameters, likelihood, AIC/BIC, prediction accuracy\n",
    "- `fitting_result_cross_validaiton` (if requested): Test accuracies across folds\n",
    "\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f3ff77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# simulate data\n",
    "forager_RW = ForagerQLearning(number_of_learning_rate=1,number_of_forget_rate=0,choice_kernel=\"none\",action_selection=\"epsilon-greedy\")\n",
    "ground_truth_params = forager_RW.params.model_dump()\n",
    "print(ground_truth_params)\n",
    "task = UncoupledBlockTask(reward_baiting=False, num_trials=100, seed=42)\n",
    "forager_RW.perform(task)\n",
    "choice_history = forager_RW.get_choice_history()\n",
    "reward_history = forager_RW.get_reward_history()\n",
    "ground_truth_q_value = forager_RW.q_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa61c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a new model\n",
    "forager_RW.fit(\n",
    "    choice_history,\n",
    "    reward_history,\n",
    "    clamp_params={\"biasL\": 0},\n",
    "    DE_kwargs=dict(workers=4, disp=True, seed=np.random.default_rng(42)),\n",
    "    k_fold_cross_validation=None,\n",
    ")\n",
    "\n",
    "fitting_result = forager_RW.fitting_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d98d095",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check fitting performance and results\n",
    "fit_names = fitting_result.fit_settings[\"fit_names\"]\n",
    "ground_truth = [num for name, num in ground_truth_params.items() if name in fit_names]\n",
    "print(f\"Num of trials: {len(choice_history)}\")\n",
    "print(f\"Fitted parameters: {fit_names}\")\n",
    "print(f'Ground truth: {[f\"{num:.4f}\" for num in ground_truth]}')\n",
    "print(f'Fitted:       {[f\"{num:.4f}\" for num in fitting_result.x]}')\n",
    "print(f\"Likelihood-Per-Trial: {fitting_result.LPT}\")\n",
    "print(f\"Prediction accuracy full dataset: {fitting_result.prediction_accuracy}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8687ae4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the fitted session results\n",
    "print(f\"Fitted parameters: {fit_names}\")\n",
    "print(f'Ground truth: {[f\"{num:.4f}\" for num in ground_truth]}')\n",
    "print(f'Fitted:       {[f\"{num:.4f}\" for num in fitting_result.x]}')\n",
    "fig_fitting, axes = forager_RW.plot_fitted_session(if_plot_latent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf07482",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Fitted parameters: {fit_names}\")\n",
    "print(f'Ground truth: {[f\"{num:.4f}\" for num in ground_truth]}')\n",
    "print(f'Fitted:       {[f\"{num:.4f}\" for num in fitting_result.x]}')\n",
    "fig_fitting, axes = forager_RW.plot_fitted_session(if_plot_latent=True)\n",
    "xx = np.arange(1, len(ground_truth_q_value[0]) + 1)\n",
    "axes[0].plot(xx, ground_truth_q_value[0], lw=1, color=\"red\", ls=\"-\", label=\"actual_Q(L)\")\n",
    "axes[0].plot(xx, ground_truth_q_value[1], lw=1, color=\"blue\", ls=\"-\", label=\"actual_Q(R)\")\n",
    "axes[0].legend(fontsize=6, loc=\"upper left\", bbox_to_anchor=(0.6, 1.3), ncol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786f4195",
   "metadata": {},
   "source": [
    "**Question**: Why the inferred learning rate/epsilon don't match ground truth but the fitted latent variables still look the same? Can you resolve this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09c1d97",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b> Answer </b></summary>\n",
    "Insufficient trial numbers AND indistinguishable loss landscape\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad45c1e",
   "metadata": {},
   "source": [
    "Let's visualize how model fitness change with respect to parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49d4214",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_worker = int(mp.cpu_count() / 2)\n",
    "compute_LL_surface('RW1972_epsi', ['epsilon', 'learn_rate'], [[0,0],[1,1]],\n",
    "                   true_para = [0.5,0.1], n_trials = 100,\n",
    "                   fit_method = 'L-BFGS-B', n_x0s = 8, pool=mp.Pool(processes = n_worker))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afe1c9b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Example: win-stay-lose-shift\n",
    "<!-- \n",
    "\n",
    "The win-stay-lose-shift model is one of the simplest models that adapts its behavior according to feedback. Consistent with the name, the model repeats rewarded actions and switches away from unrewarded actions.\n",
    "\n",
    "In the noisy version of the model, the win-stay-lose-shift rule is applied probabilistically, such that the model applies the win-stay-lose-shift rule with probability $1−\\epsilon$, and chooses randomly with probability $\\epsilon$. In the two-bandit case, the probability of choosing option k is: \n",
    "\n",
    "Let $c_t \\in \\{1, 2\\}$ be the choice at trial $t$, and $r_t \\in \\{0, 1\\}$ the reward at trial $t$. Then:\n",
    " -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b773f390",
   "metadata": {},
   "source": [
    "Now let's initialize an artificial agent with biased 'win-stay-lose-shift' algorithm with `ForagerLossCounting` class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad823870",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b> class ForagerLossCounting explained </b></summary>\n",
    "\n",
    "**Overview:**\n",
    "\n",
    "The `ForagerLossCounting` class implements a simple, psychologically plausible agent that uses a loss-counting strategy to decide when to switch actions in a dynamic foraging task. Instead of tracking action values, the agent counts consecutive unrewarded trials (losses) for the current choice. When the loss count exceeds a threshold, the agent switches to the other option.\n",
    "\n",
    "**Inputs**\n",
    "- `win_stay_lose_switch`: Boolean, enables classic win-stay-lose-shift logic (default True). If true, `loss_count_threshold_mean` and `loss_count_threshold_std` are fixed at 1 and 0\n",
    "- `choice_kernel`: String, controls influence of choice history (\"none\", \"one_step\", \"full\").\n",
    "- `params`: dictionary with following keys\n",
    "    - `loss_count_threshold_mean`: Mean threshold for switching after losses.\n",
    "    - `loss_count_threshold_std`: Standard deviation for threshold (adds stochasticity).\n",
    "    - `biasL`: Optional bias toward left choice.\n",
    "\n",
    "**Unique Attributes**\n",
    "- `loss_count`: number of consecutive unrewarded trials\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b60647f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialize the task\n",
    "task = UncoupledBlockTask(reward_baiting=True, num_trials=100, seed=42)\n",
    "# initialize the forager with 'win-stay-lose-shift' algorithm\n",
    "forager_LC = ForagerLossCounting(win_stay_lose_switch=True, choice_kernel='none')\n",
    "# fix some parameters\n",
    "forager_LC.set_params(biasL=0.1)  \n",
    "# extract the parameters\n",
    "forager_LC.params.model_dump() # biasL term is not used here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b5ea51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "forager_LC.perform(task)\n",
    "\n",
    "# Capture the results\n",
    "ground_truth_params = forager_LC.params.model_dump()\n",
    "ground_truth_loss_count = forager_LC.loss_count\n",
    "ground_truth_choice_prob = forager_LC.choice_prob\n",
    "\n",
    "# Get the history\n",
    "choice_history = forager_LC.get_choice_history()\n",
    "reward_history = forager_LC.get_reward_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f821d3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the session results\n",
    "fig, axes = forager_LC.plot_session(if_plot_latent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a53b81",
   "metadata": {},
   "source": [
    "Notice the `loss_count` trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e3b5a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fit the model to recover parameters\n",
    "forager_LC_new = ForagerLossCounting(win_stay_lose_switch=True, choice_kernel='none')\n",
    "forager_LC_new.fit(\n",
    "    choice_history,\n",
    "    reward_history,\n",
    "    DE_kwargs=dict(workers=4, disp=True, seed=np.random.default_rng(42)),\n",
    "    k_fold_cross_validation=None,\n",
    ")\n",
    "\n",
    "fitting_result = forager_LC_new.fitting_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4b091a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check fitted parameters\n",
    "fit_names = fitting_result.fit_settings[\"fit_names\"]\n",
    "print(fit_names)\n",
    "ground_truth = [num for name, num in ground_truth_params.items() if name in fit_names]\n",
    "print(f\"Num of trials: {len(choice_history)}\")\n",
    "print(f\"Fitted parameters: {fit_names}\")\n",
    "print(f'Ground truth: {[f\"{num:.4f}\" for num in ground_truth]}')\n",
    "print(f'Fitted:       {[f\"{num:.4f}\" for num in fitting_result.x]}')\n",
    "print(f\"Likelihood-Per-Trial: {fitting_result.LPT}\")\n",
    "print(f\"Prediction accuracy full dataset: {fitting_result.prediction_accuracy}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d545785e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the fitted session results\n",
    "fig_fitting, axes = forager_LC_new.plot_fitted_session(if_plot_latent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ae0d8b-d9db-4ad9-8ef0-e2ed9b036ecf",
   "metadata": {},
   "source": [
    "### Recover parameter sets (Optional)\n",
    "\n",
    "By simulating agents with a range of parameter combinations from a fixed model type, we can assess the identifiability and robustness of the model—determining whether the fitting procedure can reliably recover true parameters across diverse scenarios. This process is crucial for validating that the model is not only flexible enough to capture behavioral variability.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ba6830",
   "metadata": {},
   "source": [
    "To do this, we can follow these steps: \n",
    "\n",
    "- Choose the model(s) $M_1, M_2, \\dots$ you want to test (e.g. Rescorla–Wagner, WSLS).  \n",
    "- For each model, define parameter ranges: e.g. Learning rate $\\alpha \\in [0.05, 0.9]$, inverse temperature $\\sigma \\in [0.5, 10]$  \n",
    "\n",
    "- Simulate Behavioral Data: for every parameter draw $\\boldsymbol{\\theta}^{(i)}$:  \n",
    "     1. Initialize model state (e.g., $Q^k_0 = 0$).  \n",
    "     2. Loop over trials $t = 1 \\dots T$:  \n",
    "        - Sample choice $c_t \\sim p(c_t \\mid d_{1:t-1}, \\boldsymbol{\\theta}^{(i)}, M)$  \n",
    "        - Sample outcome $r_t$ from the task’s reward schedule  \n",
    "        - Update model states.  \n",
    "\n",
    "- Compute Diagnostic Summaries such as choice-probability curves, use these summaries to confirm the simulated behavior matches the qualitative patterns you expect.\n",
    "\n",
    "- Parameter-Recovery Check: fit the same model back to the simulated data. Good recovery (slope ≈ 1, $R^2$ high) indicates your model + fitting procedure can, in principle, retrieve true parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baae4cb1",
   "metadata": {},
   "source": [
    "#### Example: Loss Counting Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe63ba5",
   "metadata": {},
   "source": [
    "Generate multiple LossCounting model with different parameter sets. `fit_para_recovery` is a pre-packed function to do this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f2d327",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b> function fit_para_recovery explained </b></summary>\n",
    "\n",
    "**Overview**\n",
    "Generate fake data with given parameter and run model fitting to the fake data\n",
    "\n",
    "**Inputs**\n",
    "- `forager`:\n",
    "     - str, model type identifier: e.g. `LossCounting` or `RW1972_softmax`, full list refer to the [dict] `forager_params` defined below, which maps forager names to their parameter lists. `learn_rate` has to be included for `RW1972` kind of model\n",
    "- `para_names`: list[str]\n",
    "- `para_bounds`: list[str], bound of optization range \n",
    "- `para_scales`: scales to display the plot (only for visualization)\n",
    "- `true_paras`: numpy array \n",
    "- `fit_method`: str, Optimization algorithm: e.g. `DE` or `L-BFGS-B`\n",
    "\n",
    "**Outputs**\n",
    "- `true_paras`:\tnumpy.ndarray, (n_parameters, n_models), True parameter values used for simulation\n",
    "- `fitted_paras`:numpy.ndarray, (n_parameters, n_models), Recovered parameter values from fitting\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809b8ac6-82b1-4632-afef-831ce4e128cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mapping between forager names and their parameters\n",
    "forager_params = {\n",
    "    'RW1972_softmax': ['learn_rate', 'softmax_temperature', 'biasL', 'forget_rate'],\n",
    "    'RW1972_epsi': ['epsilon', 'learn_rate', 'biasL', 'forget_rate'],\n",
    "    'LossCounting': ['loss_count_threshold_mean', 'loss_count_threshold_std', 'biasL', 'choice_kernel_step_size', 'choice_kernel_relative_weight'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1de592",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# choose a model and generate a set of ground truth parameters \n",
    "n_trials = 400\n",
    "forager_name = 'LossCounting'\n",
    "para_names = ['loss_count_threshold_mean','loss_count_threshold_std']\n",
    "para_bounds = [[0,0],[30,2]] # specify parameter range \n",
    "true_paras = generate_true_paras(para_bounds=[[0,0],[30,1]], n_models = 10, method = 'random_uniform')\n",
    "\n",
    "#visualization of the true parameters\n",
    "# fig, axs = plt.subplots(1, 1, figsize=(4, 4), dpi=200)\n",
    "# axs.plot(true_paras[0,:], true_paras[1,:], 'o')\n",
    "# axs.set_xlabel('loss_count_threshold_mean')\n",
    "# axs.set_ylabel('loss_count_threshold_std')\n",
    "# axs.set_title('True parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06965ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fit model to recover parameters (this function takes some time to compute)\n",
    "\n",
    "n_worker = int(mp.cpu_count()/2)\n",
    "pool = mp.Pool(processes = n_worker)\n",
    "print(n_worker)\n",
    "    \n",
    "fit_para_recovery(forager = forager_name, \n",
    "                  para_names = para_names, para_bounds = para_bounds, \n",
    "                  true_paras = true_paras, n_trials = n_trials, \n",
    "                  fit_method = 'DE', pool = pool);    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556a35dd",
   "metadata": {},
   "source": [
    "Compute the loss likelihood landscape for different parameter combinations. This can be used to examine how different parameters contribute the behavioral output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a26a164",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# visualize behavior of the ground truth agent and the fitted agent \n",
    "n_trials = 300\n",
    "forager_name = 'LossCounting'\n",
    "para_names = ['loss_count_threshold_mean','loss_count_threshold_std']\n",
    "para_bounds = [[0,0],[30,1]]\n",
    "true_para = [10,1]\n",
    "\n",
    "# LL_surface\n",
    "compute_LL_surface(forager_name, para_names, para_bounds,\n",
    "                   true_para = true_para, n_trials = n_trials,\n",
    "                   fit_method = 'L-BFGS-B', n_x0s = 8, pool=pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073500cd",
   "metadata": {},
   "source": [
    "#### Example: Rescola-Wagner with Softmax "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ae9e4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_trials = 300\n",
    "\n",
    "forager_name = 'RW1972_softmax'\n",
    "para_names = ['learn_rate','softmax_temperature','biasL']\n",
    "para_scales = ['linear','log', 'linear']\n",
    "para_bounds = [[0,1e-10,-5],\n",
    "               [1, 1, 5]]\n",
    "\n",
    "n_models = 10\n",
    "true_paras = np.vstack((np.random.uniform(0, 1, size = n_models),\n",
    "                        1/np.random.exponential(10, size = n_models),\n",
    "                        np.random.uniform(-5, 5, size = n_models))) # Inspired by Wilson 2019. I found beta ~ Exp(10) would be better\n",
    "true_paras, fitted_para = fit_para_recovery(forager_name, \n",
    "              para_names, para_bounds, true_paras, n_trials = n_trials, \n",
    "              para_scales = para_scales, para_color_code = 2, para_2ds = [[0,2],[1,2]],\n",
    "              fit_method = 'DE', pool = pool); "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1fa3a5-4337-47fb-be41-d0dfe3a1a74f",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "## Step3: Model Recovery\n",
    "   - Fit *all* candidate models to each surrogate dataset.  \n",
    "   - Evaluate with model fitness metric like AIC or BIC:  \n",
    "   - Ideally, the data generated by model $M_i$ is most strongly favored when you fit $M_i$ (diagonal dominance in a confusion matrix).\n",
    "\n",
    "<!-- Once you’ve fit several models to the same dataset, the next step is to determine which model provides the best explanation of the behavior. -->\n",
    "\n",
    "<!-- \n",
    "#### Approximate Posterior over Models  (need Han's explanation)\n",
    "\n",
    "The relative probability of a model given the data can then be approximated as:\n",
    "\n",
    "$$p(M \\mid \\text{data}) \\propto \\exp\\left(-\\frac{\\text{IC}}{2}\\right)$$\n",
    "\n",
    "This allows us to derive:\n",
    "- Relative log-likelihood: $\\log_{10} \\frac{p(\\text{model})}{p(\\text{best model})}$\n",
    "- Model weights: normalized probabilities over models -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971ecd4f",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> Model fitness metric </summary>\n",
    "\n",
    "- $ \\text{AIC} = -2\\log\\hat{L} + 2k$ \n",
    "- $\\text{BIC} = -2\\log\\hat{L} + k\\log T. $\n",
    "- $\\hat{L}$ is the maximum likelihood, $k$ is the number of free parameters in the model, $T$ is the number of data points (e.g., trials). Lower AIC/BIC means better model. \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cc6ea1",
   "metadata": {},
   "source": [
    "### Example: Rescorla Wagner Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7430156",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b> function BanditModelComparison explained </b></summary> \n",
    "\n",
    "**Overview**\n",
    "\n",
    "The `BanditModelComparison` class is a comprehensive tool for comparing multiple foraging models on the same behavioral data. It provides a unified interface for fitting, evaluating, and comparing different computational models of decision-making behavior in multi-armed bandit tasks.\n",
    "\n",
    "**Inputs:**\n",
    "- `choice_history`: Array of choices made by the subject (shape: [1, n_trials])\n",
    "- `reward_history`: Array of rewards received (shape: [1, n_trials])\n",
    "- `p_reward`: Optional reward probability schedule for plotting/validation\n",
    "- `session_num`: Optional session identifier for pooling across multiple sessions\n",
    "- `models`: List of models to compare (see Model Specification below)\n",
    "\n",
    "**Model Specification Options:**\n",
    "1. `None`: Uses all pre-defined models from `MODELS` list\n",
    "2. List of integers: Selects models by index from pre-defined list (e.g., `[1, 3, 5]`)\n",
    "3. Custom model list: Direct specification of model parameters\n",
    "\n",
    "**Core Functions**\n",
    "\n",
    "1. Model Fitting `fit()`\n",
    "\n",
    "Parameters:\n",
    "- `fit_method`: Optimization algorithm ('DE' for differential evolution)\n",
    "- `fit_settings`: Dictionary of optimization parameters\n",
    "- `pool`: Multiprocessing pool for parallel fitting\n",
    "- `if_verbose`: Whether to print progress information\n",
    "- `plot_predictive`: Which models to plot predictive traces for\n",
    "\n",
    "Output:\n",
    "- `self.results`: Pandas DataFrame with fitting results for all models\n",
    "- `self.results_sort`: Results sorted by AIC (best model first)\n",
    "\n",
    "2. Cross-Validation `cross_validate()`\n",
    "\n",
    "3. Visualization Methods\n",
    "\n",
    "`plot_predictive_choice()`\n",
    "- Plots predictive choice probability traces for fitted models\n",
    "- Shows how well each model predicts the subject's choices over time\n",
    "\n",
    "`plot()`\n",
    "- Creates comprehensive model comparison plots\n",
    "- Includes AIC/BIC comparisons, parameter estimates, and model weights\n",
    "\n",
    "\n",
    "**Model Comparison Metrics**\n",
    "\n",
    "1. AIC (Akaike Information Criterion):\n",
    "- Balances model fit with complexity\n",
    "- Lower values indicate better models\n",
    "- Formula: AIC = 2K - 2ln(L), where K = number of parameters, L = likelihood\n",
    "\n",
    "2. BIC (Bayesian Information Criterion):\n",
    "- More stringent penalty for model complexity\n",
    "- Lower values indicate better models\n",
    "- Formula: BIC = ln(n)K - 2ln(L), where n = number of observations\n",
    "\n",
    "3. Model Weight:\n",
    "- Normalized relative likelihood (sums to 1)\n",
    "- Can be interpreted as posterior probability of each model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17d410c-d90e-40c2-b5ad-173154bb7016",
   "metadata": {
    "tags": []
   },
   "source": [
    "<details>\n",
    "<summary> List of available models and parameters for dynamic foraging task </summary>\n",
    "\n",
    "\n",
    "```python\n",
    "# [model_name, para_names, [low bounds], [upper bounds]]\n",
    "MODELS = [\n",
    "            # No bias (1-8)\n",
    "            ['LossCounting', ['loss_count_threshold_mean', 'loss_count_threshold_std'], [0,0], [40,10]],                   \n",
    "            ['RW1972_epsi', ['learn_rate', 'epsilon'],[0, 0],[1, 1]],\n",
    "            ['LNP_softmax',  ['tau1', 'softmax_temperature'], [1e-3, 1e-2], [100, 15]],                 \n",
    "            ['LNP_softmax', ['tau1', 'tau2', 'w_tau1', 'softmax_temperature'],[1e-3, 1e-1, 0, 1e-2],[15, 40, 1, 15]],                 \n",
    "            ['RW1972_softmax', ['learn_rate', 'softmax_temperature'],[0, 1e-2],[1, 15]],\n",
    "            ['Hattori2019', ['learn_rate_rew', 'learn_rate_unrew', 'softmax_temperature'],[0, 0, 1e-2],[1, 1, 15]],\n",
    "            ['Bari2019', ['learn_rate', 'forget_rate', 'softmax_temperature'],[0, 0, 1e-2],[1, 1, 15]],\n",
    "            ['Hattori2019', ['learn_rate_rew', 'learn_rate_unrew', 'forget_rate', 'softmax_temperature'],[0, 0, 0, 1e-2],[1, 1, 1, 15]],\n",
    "            \n",
    "            # With bias (9-15)\n",
    "            ['RW1972_epsi', ['learn_rate', 'epsilon', 'biasL'],[0, 0, -0.5],[1, 1, 0.5]],\n",
    "            ['LNP_softmax',  ['tau1', 'softmax_temperature', 'biasL'], [1e-3, 1e-2, -5], [100, 15, 5]],                 \n",
    "            ['LNP_softmax', ['tau1', 'tau2', 'w_tau1', 'softmax_temperature', 'biasL'],[1e-3, 1e-1, 0, 1e-2, -5],[15, 40, 1, 15, 5]],                 \n",
    "            ['RW1972_softmax', ['learn_rate', 'softmax_temperature', 'biasL'],[0, 1e-2, -5],[1, 15, 5]],\n",
    "            ['Hattori2019', ['learn_rate_rew', 'learn_rate_unrew', 'softmax_temperature', 'biasL'],[0, 0, 1e-2, -5],[1, 1, 15, 5]],\n",
    "            ['Bari2019', ['learn_rate', 'forget_rate', 'softmax_temperature', 'biasL'],[0, 0, 1e-2, -5],[1, 1, 15, 5]],\n",
    "            ['Hattori2019', ['learn_rate_rew', 'learn_rate_unrew', 'forget_rate', 'softmax_temperature', 'biasL'],[0, 0, 0, 1e-2, -5],[1, 1, 1, 15, 5]],\n",
    "            \n",
    "            # With bias and choice kernel (16-21)\n",
    "            ['LNP_softmax_CK',  ['tau1', 'softmax_temperature', 'biasL', 'choice_step_size','choice_softmax_temperature'], \n",
    "                             [1e-3, 1e-2, -5, 0, 1e-2], [100, 15, 5, 1, 20]],                 \n",
    "            ['LNP_softmax_CK', ['tau1', 'tau2', 'w_tau1', 'softmax_temperature', 'biasL', 'choice_step_size','choice_softmax_temperature'],\n",
    "                             [1e-3, 1e-1, 0, 1e-2, -5, 0, 1e-2],[15, 40, 1, 15, 5, 1, 20]],                 \n",
    "            ['RW1972_softmax_CK', ['learn_rate', 'softmax_temperature', 'biasL', 'choice_step_size','choice_softmax_temperature'],\n",
    "                             [0, 1e-2, -5, 0, 1e-2],[1, 15, 5, 1, 20]],\n",
    "            ['Hattori2019_CK', ['learn_rate_rew', 'learn_rate_unrew', 'softmax_temperature', 'biasL', 'choice_step_size','choice_softmax_temperature'],\n",
    "                             [0, 0, 1e-2, -5, 0, 1e-2],[1, 1, 15, 5, 1, 20]],\n",
    "            ['Bari2019_CK', ['learn_rate', 'forget_rate', 'softmax_temperature', 'biasL', 'choice_step_size','choice_softmax_temperature'],\n",
    "                             [0, 0, 1e-2, -5, 0, 1e-2],[1, 1, 15, 5, 1, 20]],\n",
    "            ['Hattori2019_CK', ['learn_rate_rew','learn_rate_unrew', 'forget_rate','softmax_temperature', 'biasL', 'choice_step_size','choice_softmax_temperature'],\n",
    "                               [0, 0, 0, 1e-2, -5, 0, 1e-2],[1, 1, 1, 15, 5, 1, 20]],\n",
    "            # ['Hattori2019_CK', ['learn_rate_rew','learn_rate_unrew', 'forget_rate','softmax_temperature', 'biasL', 'choice_step_size','choice_softmax_temperature'],\n",
    "            #                    [0, 0, 0, 1e-2, -5, 1, 1e-2],[1, 1, 1, 15, 5, 1, 20]],  # choice_step_size fixed at 1 --> Bari 2019: only the last choice matters\n",
    "            \n",
    "         ]\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce23314",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "choice_history, reward_history, p_reward = generate_fake_data('RW1972_epsi', ['learn_rate','epsilon'], [0.3, 0.2])\n",
    "model_comparison = BanditModelComparison(choice_history, reward_history, p_reward, models = [1,2,5])\n",
    "model_comparison.fit(pool = mp.Pool(processes=n_worker), plot_predictive=[1]) # Plot predictive traces for the 1st, 2nd, and 3rd models\n",
    "model_comparison.show()\n",
    "model_comparison.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7775fcb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Models Fit to Real Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febc2d8e-c5d2-40c5-9825-e2f5171b2681",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we initialize a Q learning forager \n",
    "forager_RW = ForagerQLearning(number_of_learning_rate=1,number_of_forget_rate=0, choice_kernel = 'one_step', action_selection = 'epsilon-greedy')\n",
    "# Fit the model to recover parameters\n",
    "forager_RW.fit(\n",
    "    choice_history_real_data,\n",
    "    reward_history_real_data,\n",
    "    DE_kwargs=dict(workers=4, disp=True, seed=np.random.default_rng(42)),\n",
    "    k_fold_cross_validation=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2059cfbe-923b-4609-a891-1e951b90c45a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "forager_RW.plot_fitted_session(if_plot_latent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f05c0a6",
   "metadata": {},
   "source": [
    "# Potential projects\n",
    "- Fit multiple models to real data\n",
    "- Select a different task environment\n",
    "- Explore the behavioral model list provided but not introduced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd2679d",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- Wilson, R. C., & Collins, A. G. (2019). Ten simple rules for the computational modeling of behavioral data. *eLife*, 8, e49547. https://doi.org/10.7554/eLife.49547  \n",
    "- Palminteri, S., Wyart, V., & Koechlin, É. (2017). The importance of falsification in computational cognitive modeling. *Trends in Cognitive Sciences*, 21(6), 425–433. https://doi.org/10.1016/j.tics.2017.03.011\n",
    "- (Rescorla & Wagner, 1972)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
